{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning Group - Project\n",
    "This is an explanatory Jupyter Notebook that is intended to show how the Project Group approached the \"Solar Energy Production Forecasting\" challenge. \n",
    "\n",
    "### Table of Contents:\n",
    " 1. [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA)) \n",
    " 2. [Feature Extraction](#Feature-Extraction) \n",
    " 3. [Machine Learning Pipeline](#Machine-Learning-Pipeline) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data Processing Tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning Models\n",
    "import catboost as cb\n",
    "\n",
    "# Machine Learning Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_length_matching(train: pd.DataFrame, obs: pd.DataFrame)-> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function is intended to ensure that both the training data and\n",
    "    the observed data are sorted, and contain the same number of entries. \n",
    "    \"\"\"\n",
    "\n",
    "    # Cut the data frames so that their date match.\n",
    "    obs_feature_test = obs[obs['date_forecast'].isin(train['time'])].sort_values(by=['date_forecast'])  # sortert etter datao\n",
    "    # If only one of them has the date ensure that the other also has the same sorting.\n",
    "    train_feature_test = train[train['time'].isin(obs['date_forecast'])].sort_values(by=['time'])       # sortert etter datao\n",
    "\n",
    "    return train_feature_test, obs_feature_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]):\n",
    "    squared_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        squared_df['squared_' + measurement + '_2'] = df[measurement]**2\n",
    "    return squared_df\n",
    "\n",
    "def difference_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    der_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        der_df['derivative_' + measurement + '_d'] = df[measurement].diff()\n",
    "    \n",
    "    df = df.drop('time_diff', axis =  1)\n",
    "\n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    der_df = der_df.interpolate(method='linear')\n",
    "    der_df = der_df.bfill()\n",
    "    \n",
    "    return der_df\n",
    "\n",
    "def double_derivative_from_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    dder_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    # Calculate derivative estimates\n",
    "    for measurement in measurements:\n",
    "        dder_df['double_derivative_' + measurement + '_dd'] = df[measurement].diff() / (divmod(df['time_diff'].dt.total_seconds(), 60)[0]**2)\n",
    "    \n",
    "    df = df.drop('time_diff', axis=1)\n",
    "    \n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    dder_df = dder_df.interpolate(method='linear')\n",
    "    dder_df = dder_df.bfill()\n",
    "\n",
    "    return dder_df\n",
    "\n",
    "def daily_accumulated_val_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    i_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        i_df['integral_' + measurement + '_integral'] = df.groupby('date')[measurement].cumsum()\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return i_df\n",
    "\n",
    "def daily_accumulated_val_squared_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    di_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        di_df['double_integral_' + measurement + '_dintegral'] = df.groupby('date')[measurement].cumsum()**2\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return di_df\n",
    "\n",
    "def time_data_from_df(df: pd.DataFrame, timestamps: str) -> pd.DataFrame: \n",
    "    # Extracting components\n",
    "    time_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    time_df['day_of_year:day'] = df[timestamps].dt.dayofyear\n",
    "    time_df['month:month'] = df[timestamps].dt.month\n",
    "    #time_df['year:year'] = df[timestamps].dt.year\n",
    "    time_df['hour:hour'] = df[timestamps].dt.hour\n",
    "    return time_df\n",
    "\n",
    "\n",
    "\n",
    "def n_largest_freq(df: pd.DataFrame, measurements: list[str],n_largest: int):\n",
    "    \"\"\"\n",
    "    Generates values based on the largest frequencies that are present.\n",
    "    \"\"\"\n",
    "    for measurement in measurements:\n",
    "        signal = df[measurement].values\n",
    "\n",
    "        fft_result = np.fft.fft(signal)\n",
    "        \n",
    "        # Keep only the dominant frequencies (e.g., top 5)\n",
    "        num_components_to_keep = n_largest\n",
    "\n",
    "        indices = np.argsort(np.abs(fft_result))[::-1][:num_components_to_keep]\n",
    "\n",
    "        # Set all other frequency components to zero\n",
    "        fft_result_filtered = np.zeros_like(fft_result)\n",
    "        fft_result_filtered[indices] = fft_result[indices]\n",
    "\n",
    "        # Compute IFFT\n",
    "        ifft_result = np.fft.ifft(fft_result_filtered)\n",
    "\n",
    "        # Add the filtered results to the dataframe\n",
    "        df[\"filtered_\" + measurement] = ifft_result.real\n",
    "\n",
    "    return df\n",
    "\n",
    "def freq_comb(df: pd.DataFrame, features: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Takes the fourier transform of multiple signals add them together, and then takes the inverse.\n",
    "\n",
    "    features: Are what you would like to combine.\n",
    "    df: Chosen dataframe containing feature information.\n",
    "    \"\"\"\n",
    "\n",
    "    total_fft = 0\n",
    "    \n",
    "    for feat in features:\n",
    "        # Finding the signal directly might be wrong due to timestamps and such, but might still be helpful. It is not correct, but improvements like day by day sampling might be useful.\n",
    "        signal = df[feat].values\n",
    "\n",
    "        # Min-max scaling\n",
    "        scaled_signal = min_max_scale(signal)\n",
    "        \n",
    "        fft = np.fft.fft(scaled_signal)\n",
    "        total_fft = total_fft + fft\n",
    "    \n",
    "    ifft_result = np.fft.ifft(total_fft)\n",
    "\n",
    "    return ifft_result.real\n",
    "\n",
    "def min_max_scale(signal: np.array) -> np.array:\n",
    "    # Calculate min and max values\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "\n",
    "    # Min-max scaling\n",
    "    scaled_signal = (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_signal\n",
    "\n",
    "def shifted_values_24_h(y: pd.DataFrame, measurement: str)->pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1, 25):\n",
    "        df[measurement + 'n-' + str(i)] = y[measurement].shift(i)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(df: pd.DataFrame):\n",
    "    # Extract the part before \":\" in column names\n",
    "    df.columns = df.columns.str.split(':').str[0]\n",
    "\n",
    "    # Group by modified column names and sum values\n",
    "    grouped_df = df.groupby(df.columns, axis=1).sum()\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "Testing different Machine Learning models, and tuning and adjusting features quickly becomes a bother if one does not create helper function and classes. The code bellow is what the Project Group developed during the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A couple of functions to generate the approprate features for both training and prediction data.\n",
    "\"\"\"\n",
    "def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], add_feedback: bool = False):\n",
    "   \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "    measurements = list(X.columns.values)\n",
    "    measurements.remove(timestamps)\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X, timestamps, measurements)\n",
    "    der_df = difference_df(X, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "    time_df = time_data_from_df(X, timestamps)\n",
    "\n",
    "    X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "    if len(filter_list) > 0:\n",
    "        X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "    # Additional features\n",
    "    der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "    der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "    y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "\n",
    "    # Adding together the added features to one dataframe.\n",
    "    y_BIG = pd.concat([y, der_y_shifted, y_shifted])\n",
    "\n",
    "\n",
    "    # Making sure that the two dataframes match in length.\n",
    "    y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "    # Get our desired output\n",
    "    y = y_BIG[\"pv_measurement\"]\n",
    "    y = y.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "    if add_feedback:\n",
    "        # Removing datetime object column.\n",
    "        y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "        y_features = y_features.drop('time', axis=1)\n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "\n",
    "        X = pd.concat([X, y_features], axis = 1)\n",
    "    \n",
    "    # Removing datetime object column\n",
    "    X = X.drop(timestamps, axis=1)\n",
    "    X = X.reset_index(drop = True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def pred_data_processing(X_pred: pd.DataFrame, filter_list: list[str] = []) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that reads\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X_pred = X_pred.interpolate(method = 'linear')\n",
    "    X_pred = X_pred.bfill()\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "\n",
    "    # Removing date-time from measurements\n",
    "    measurements = list(X_pred.columns.values)\n",
    "    measurements.remove(\"date_forecast\")\n",
    "    measurements.remove(\"date_calc\")\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X_pred, timestamps, measurements)\n",
    "    der_df = difference_df(X_pred, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "    time_df = time_data_from_df(X_pred, timestamps)\n",
    "\n",
    "    \n",
    "\n",
    "    X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "    if len(filter_list) > 0:\n",
    "        X_pred_new = X_pred_new[filter_list + ['date_forecast']]\n",
    "    else:\n",
    "        X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "\n",
    "    return X_pred_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to organize the different steps in the machine learning pipeline. The class contains some nice helper functions\n",
    "that helps the user gain insight into what features the model finds the most usefull.\n",
    "\"\"\"\n",
    "\n",
    "class learner:\n",
    "    def __init__(self, file_paths: list[list[str]], features: list[str] = [], learning_algorithm: str = cb.CatBoostRegressor) -> None:\n",
    "        self.file_paths = file_paths\n",
    "        self.features = features\n",
    "        self.learning_algorithm = learning_algorithm\n",
    "        self.buildings = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "    def create_training_data_multi_model(self):\n",
    "        self.X_train_sets = []\n",
    "        self.X_test_sets = []\n",
    "        self.y_train_sets = []\n",
    "        self.y_test_sets = []\n",
    "        self.X_pred_sets = []\n",
    "\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            y = pd.read_parquet(path[0])\n",
    "            X_estimated = pd.read_parquet(path[1])\n",
    "            X_observed = pd.read_parquet(path[2])\n",
    "            X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred = pred_data_processing(X_pred, self.features)\n",
    "            X_pred['building'] = i\n",
    "            \n",
    "            \n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            # Pre-process data\n",
    "            y = y.dropna()\n",
    "            X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "            X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "            # BETTER NAME\n",
    "            X, y= train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = i\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "\n",
    "            self.X_train_sets.append(X_train)\n",
    "            self.X_test_sets.append(X_test)\n",
    "            self.y_train_sets.append(y_train)\n",
    "            self.y_test_sets.append(y_test)\n",
    "            self.X_pred_sets.append(X_pred)\n",
    "\n",
    "\n",
    "    def create_training_data(self):\n",
    "        list_y = []\n",
    "        list_X = []\n",
    "        list_X_pred = []\n",
    "        scalers = []\n",
    "    \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            y = pd.read_parquet(path[0])\n",
    "            X_estimated = pd.read_parquet(path[1])\n",
    "            X_observed = pd.read_parquet(path[2])\n",
    "            X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred = dp.pred_data_processing(X_pred, self.features)\n",
    "            X_pred['building'] = i\n",
    "            \n",
    "            list_X_pred.append(X_pred)\n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            # Pre-process data\n",
    "            y = y.dropna()\n",
    "            X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "            X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "            # BETTER NAME\n",
    "            X, y= dp.train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = i\n",
    "\n",
    "            list_y.append(y)\n",
    "            list_X.append(X)\n",
    "\n",
    "        self.scalers = scalers\n",
    "        # Add all the lists together. However there is a need to add set\n",
    "        y = pd.concat(list_y, axis= 0, ignore_index=True)\n",
    "        X = pd.concat(list_X, axis= 0, ignore_index=True)\n",
    "        X_pred = pd.concat(list_X_pred, axis = 0, ignore_index=True)\n",
    "        \n",
    "        y = y.reset_index(drop=True)\n",
    "        X = X.reset_index(drop=True)\n",
    "        X_pred = X_pred.reset_index(drop=True)\n",
    "\n",
    "        # Strange\n",
    "        y.plot()\n",
    "\n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "        \n",
    "        # Should probably save their date indexes just in case :/\n",
    "\n",
    "        # Should try scaling later ... :/\n",
    "        self.X_train, self.X_test, self.X_pred = X_train, X_test, X_pred\n",
    "        print(\"These are self XPREDs features\", self.X_pred.columns.values)\n",
    "        return None\n",
    "\n",
    "    def _scale_sets(self, X_train: pd.DataFrame, X_test: pd.DataFrame, X_pred: pd.DataFrame):\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        unscaled_X_train = X_train.drop('building', axis = 1)\n",
    "        X_train_scaled_values = scaler.fit_transform(unscaled_X_train.values)\n",
    "\n",
    "        X_train_scaled['building'] = X_train['building']\n",
    "\n",
    "        unscaled_X_test = X_test.drop('building', axis = 1)\n",
    "        X_test_scaled = scaler.transform(unscaled_X_test)\n",
    "        X_test_scaled['building'] = X_test['building']\n",
    "        \n",
    "        unscaled_X_pred = X_pred.drop('building', axis = 1)\n",
    "        X_pred_scaled = scaler.transform(unscaled_X_pred)\n",
    "        X_pred_scaled['building'] = X_pred['building']\n",
    "        \n",
    "        return X_train, X_test, X_pred\n",
    "\n",
    "    def fit_multi_model(self):\n",
    "        \n",
    "        self.models = []\n",
    "\n",
    "        for i in range(len(self.X_train_sets)): \n",
    "            #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "            # Add a function that picks between different models, and processes the data based on this\n",
    "            train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "\n",
    "            model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "            grid = {'iterations': [100, 150, 200],\n",
    "                    'learning_rate': [0.03, 0.1],\n",
    "                    'depth': [2, 4, 6, 8],\n",
    "                    'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "            model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict_multi_model(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            X_pred = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "            unformated_pred = self.models[i].predict(X_pred)\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.X_pred_sets[i][\"building\"]\n",
    "\n",
    "            replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def get_performance_multi_model(self) -> None:\n",
    "        mae_sum = 0\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            pred = self.models[i].predict(self.X_test_sets[i])\n",
    "            pd.Series(pred).plot()\n",
    "            pd.Series(self.y_test_sets[i]).plot()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], pred))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"Mean absolute error: \", mae_sum/len(self.X_test_sets))\n",
    "\n",
    "\n",
    "\n",
    "    def fit_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Based on the selected model the class switches between what model is doing the learning. \n",
    "        \"\"\"\n",
    "\n",
    "        #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "        # Add a function that picks between different models, and processes the data based on this\n",
    "        train_dataset = cb.Pool(self.X_train, self.y_train)\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "        grid = {'iterations': [100, 150, 200],\n",
    "                'learning_rate': [0.03, 0.1],\n",
    "                'depth': [2, 4, 6, 8],\n",
    "                'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "        self.model.grid_search(grid, train_dataset, verbose=False)\n",
    "        \n",
    "\n",
    "    def get_performance(self) -> None:\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        pd.Series(pred).plot()\n",
    "        pd.Series(self.y_test).plot()\n",
    "        \n",
    "        mae = (mean_absolute_error(self.y_test, pred))\n",
    "        print(\"Mean Abs: {:.2f}\".format(mae))\n",
    "\n",
    "    def predict(self) -> None:\n",
    "        print(self.X_pred.columns.values)\n",
    "        X_pred = self.X_pred.drop('date_forecast', axis = 1)\n",
    "        unformated_pred = self.model.predict(X_pred)\n",
    "        \n",
    "        unformated_pred_df = pd.DataFrame()\n",
    "        unformated_pred_df[\"date_forecast\"] = self.X_pred[\"date_forecast\"]\n",
    "        unformated_pred_df[\"building\"] = self.X_pred[\"building\"]\n",
    "\n",
    "        replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "        # Use the replace method with the specified column and dictionary\n",
    "        unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "        unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "        unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "    \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def save_best_features(self, filename: str, N: int):\n",
    "        best_features_df = pd.DataFrame()\n",
    "\n",
    "        for i, X in enumerate(self.X_test_sets):\n",
    "           \n",
    "            feature_importance = self.models[i].get_feature_importance()\n",
    "\n",
    "            # Pair feature names with their importance scores\n",
    "            feature_importance_dict = dict(zip(self.models[i].feature_names_, feature_importance))\n",
    "\n",
    "            # Sort features by importance\n",
    "            sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Print or use the top features\n",
    "            top_features = sorted_feature_importance[:N]  # Replace N with the number of top features you want\n",
    "            \n",
    "            # Saving to list\n",
    "            labels = list(X.columns.values)\n",
    "            best_features = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                best_features.append(feat[0])\n",
    "            \n",
    "            best_features_df[\"Model \" + self.buildings[i]] = pd.Series(np.array(best_features))\n",
    "\n",
    "        best_features_df.to_csv(filename + '.csv')\n",
    "\n",
    "\n",
    "    def _format_predictions(self, unformated_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # \n",
    "        to_be_submitted_index = pd.read_csv(\"test.csv\")\n",
    "\n",
    "        #convert the \"time\" column to datetime\n",
    "        to_be_submitted_index[\"time\"] = pd.to_datetime(to_be_submitted_index[\"time\"])\n",
    "        pred = pd.merge(unformated_pred, to_be_submitted_index, how='inner', left_on=['date_forecast', 'building'], right_on=[\"time\", \"location\"])\n",
    "        print(len(unformated_pred.index))\n",
    "        return pred\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def _save_predictions(self, pred: pd.DataFrame)->None:\n",
    "        #Make the index and pv_measurement column into a csv file\n",
    "        pred[[\"id\", \"pv_measurement\"]].rename(columns={\"id\" : \"id\" , \"pv_measurement\" : \"prediction\"}).to_csv(\"model_pred.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "              ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "              ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the user may apply their testing setup. Here they may apply what features they need, and the model will ensure the rest.\n",
    "\n",
    "TODO: Add a folder for each test applied, and add an excel document with what the project group has tried out\n",
    "TODO: Add feedback functionality.\n",
    "TODO: Add max signal functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['direct_rad:W', 'is_in_shadow:idx', 'total_cloud_cover:p', 'sun_elevation:d', \n",
    "            'derivative_direct_rad:W_d', 'derivative_sun_elevation:d_d', \n",
    "            'sun_azimuth:d', 't_1000hPa:K', \n",
    "            'derivative_sun_azimuth:d_d', 'derivative_t_1000hPa:K_d',\n",
    "            'day_of_year:day', 'month:month', 'hour:hour']\n",
    " \n",
    "\n",
    "# features = [\"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\", \"ceiling_height_agl:m\", \"clear_sky_energy_1h:J\", \n",
    "#            \"clear_sky_rad:W\", \"cloud_base_agl:m\", \"dew_or_rime:idx\", \"dew_point_2m:K\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\", \n",
    "#            \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"is_in_shadow:idx\", \"msl_pressure:hPa\", \n",
    "#            \"precip_5min:mm\", \"pressure_100m:hPa\", \"pressure_50m:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "#            \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", \"visibility:m\",\n",
    "#            \"wind_speed_10m:ms\"]\n",
    "\n",
    "\n",
    "# Should add features etc, but a bit stranve\n",
    "\n",
    "# Testing the procedure \n",
    "l = learner(file_paths = file_paths, features = features)\n",
    "l.create_training_data_multi_model()\n",
    "l.fit_multi_model()\n",
    "l.save_best_features(\"best_features\", 10)\n",
    "l.get_performance_multi_model()\n",
    "l.predict_multi_model()\n",
    "\n",
    "# IDEA: Add a different scaler for all the different features, then transform then into the right scale after the fact\n",
    "# Remove any needless features, add until you get a good outpu\n",
    "# Probable reason for the bad score is the scaling defeciencies.\n",
    "# Get the smallest value that is NoN zero, and cap them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
