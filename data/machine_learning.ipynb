{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning Group - Project\n",
    "This is an explanatory Jupyter Notebook that is intended to show how the Project Group approached the \"Solar Energy Production Forecasting\" challenge. \n",
    "\n",
    "### Table of Contents:\n",
    " 1. [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA)) \n",
    " 2. [Feature Extraction](#Feature-Extraction) \n",
    " 3. [Machine Learning Pipeline](#Machine-Learning-Pipeline) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data Processing Tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "import catboost as cb\n",
    "import h2o \n",
    "from h2o.automl import H2OAutoML \n",
    "\n",
    "\n",
    "# Machine Learning Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_length_matching(train: pd.DataFrame, obs: pd.DataFrame)-> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function is intended to ensure that both the training data and\n",
    "    the observed data are sorted, and contain the same number of entries. \n",
    "    \"\"\"\n",
    "\n",
    "    # Cut the data frames so that their date match.\n",
    "    obs_feature_test = obs[obs['date_forecast'].isin(train['time'])].sort_values(by=['date_forecast'])  # sortert etter datao\n",
    "    # If only one of them has the date ensure that the other also has the same sorting.\n",
    "    train_feature_test = train[train['time'].isin(obs['date_forecast'])].sort_values(by=['time'])       # sortert etter datao\n",
    "\n",
    "    return train_feature_test, obs_feature_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]):\n",
    "    squared_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        squared_df['squared_' + measurement + '_2'] = df[measurement]**2\n",
    "    return squared_df\n",
    "\n",
    "def upscale_(df: pd.DataFrame, feature: str, upscale: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    upscale_df = pd.DataFrame()\n",
    "    \n",
    "    upscale_df[\"uscale_\" + feature] = df[feature]*upscale\n",
    "\n",
    "    return upscale_df\n",
    "\n",
    "def dot_df(df: pd.DataFrame, dot_feature: str, features: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dot_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        dot_df[dot_feature + '_dot_' + feature] = df[dot_feature] * df[feature]\n",
    "\n",
    "    return dot_df\n",
    "\n",
    "def log_df(df: pd.DataFrame, features: list[str]):\n",
    "    \n",
    "    df = df.copy()\n",
    "    log_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        df[feature] = abs(df[feature])\n",
    "        df[feature] = df[feature] + 1\n",
    "        log_df['log_' + feature] =  np.log(df[feature])\n",
    "\n",
    "    return log_df\n",
    "\n",
    "\n",
    "def difference_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    der_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        der_df['derivative_' + measurement + '_d'] = df[measurement].diff()\n",
    "    \n",
    "    df = df.drop('time_diff', axis =  1)\n",
    "\n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    der_df = der_df.interpolate(method='linear')\n",
    "    der_df = der_df.bfill()\n",
    "    \n",
    "    return der_df\n",
    "\n",
    "def double_derivative_from_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    dder_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    # Calculate derivative estimates\n",
    "    for measurement in measurements:\n",
    "        dder_df['double_derivative_' + measurement + '_dd'] = df[measurement].diff() / (divmod(df['time_diff'].dt.total_seconds(), 60)[0]**2)\n",
    "    \n",
    "    df = df.drop('time_diff', axis=1)\n",
    "    \n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    dder_df = dder_df.interpolate(method='linear')\n",
    "    dder_df = dder_df.bfill()\n",
    "\n",
    "    return dder_df\n",
    "\n",
    "def daily_accumulated_val_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    i_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        i_df['integral_' + measurement + '_integral'] = df.groupby('date')[measurement].cumsum()\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return i_df\n",
    "\n",
    "def daily_accumulated_val_squared_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    di_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        di_df['double_integral_' + measurement + '_dintegral'] = df.groupby('date')[measurement].cumsum()**2\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return di_df\n",
    "\n",
    "def time_data_from_df(df: pd.DataFrame, timestamps: str) -> pd.DataFrame: \n",
    "    # Extracting components\n",
    "    time_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    time_df['day_of_year:day'] = df[timestamps].dt.dayofyear\n",
    "    time_df['month:month'] = df[timestamps].dt.month\n",
    "    #time_df['year:year'] = df[timestamps].dt.year\n",
    "    time_df['hour:hour'] = df[timestamps].dt.hour\n",
    "    return time_df\n",
    "\n",
    "\n",
    "\n",
    "def n_largest_freq(df: pd.DataFrame, measurements: list[str],n_largest: int):\n",
    "    \"\"\"\n",
    "    Generates values based on the largest frequencies that are present.\n",
    "    \"\"\"\n",
    "    for measurement in measurements:\n",
    "        signal = df[measurement].values\n",
    "\n",
    "        fft_result = np.fft.fft(signal)\n",
    "        \n",
    "        # Keep only the dominant frequencies (e.g., top 5)\n",
    "        num_components_to_keep = n_largest\n",
    "\n",
    "        indices = np.argsort(np.abs(fft_result))[::-1][:num_components_to_keep]\n",
    "\n",
    "        # Set all other frequency components to zero\n",
    "        fft_result_filtered = np.zeros_like(fft_result)\n",
    "        fft_result_filtered[indices] = fft_result[indices]\n",
    "\n",
    "        # Compute IFFT\n",
    "        ifft_result = np.fft.ifft(fft_result_filtered)\n",
    "\n",
    "        # Add the filtered results to the dataframe\n",
    "        df[\"filtered_\" + measurement] = ifft_result.real\n",
    "\n",
    "    return df\n",
    "\n",
    "def freq_comb(df: pd.DataFrame, features: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Takes the fourier transform of multiple signals add them together, and then takes the inverse.\n",
    "\n",
    "    features: Are what you would like to combine.\n",
    "    df: Chosen dataframe containing feature information.\n",
    "    \"\"\"\n",
    "\n",
    "    total_fft = 0\n",
    "    \n",
    "    for feat in features:\n",
    "        # Finding the signal directly might be wrong due to timestamps and such, but might still be helpful. It is not correct, but improvements like day by day sampling might be useful.\n",
    "        signal = df[feat].values\n",
    "\n",
    "        # Min-max scaling\n",
    "        scaled_signal = min_max_scale(signal)\n",
    "        \n",
    "        fft = np.fft.fft(scaled_signal)\n",
    "        total_fft = total_fft + fft\n",
    "    \n",
    "    ifft_result = np.fft.ifft(total_fft)\n",
    "\n",
    "    return ifft_result.real\n",
    "\n",
    "def min_max_scale(signal: np.array) -> np.array:\n",
    "    # Calculate min and max values\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "\n",
    "    # Min-max scaling\n",
    "    scaled_signal = (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_signal\n",
    "\n",
    "def shifted_values_24_h(y: pd.DataFrame, measurement: str)->pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1, 25):\n",
    "        df[measurement + 'n-' + str(i)] = y[measurement].shift(i)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(df: pd.DataFrame):\n",
    "    # Extract the part before \":\" in column names\n",
    "    df.columns = df.columns.str.split(':').str[0]\n",
    "\n",
    "    # Group by modified column names and sum values\n",
    "    grouped_df = df.groupby(df.columns, axis=1).sum()\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A couple of functions to generate the approprate features for both training and prediction data.\n",
    "\"\"\"\n",
    "\n",
    "def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], feedback: bool = False):\n",
    "   \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "    measurements = list(X.columns.values)\n",
    "    measurements.remove(timestamps)\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X, timestamps, measurements)\n",
    "    der_df = difference_df(X, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "    l_df = log_df(X, measurements)\n",
    "    dotted_df = dot_df(X, 'direct_rad:W', measurements)\n",
    "    time_df = time_data_from_df(X, timestamps)\n",
    "\n",
    "    X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, l_df, dotted_df, time_df], axis = \"columns\")\n",
    "\n",
    "    if len(filter_list) > 0:\n",
    "        X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "    # Additional features\n",
    "    \n",
    "    #der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "    # der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "    y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "    y.reset_index(drop = True)\n",
    "    y_shifted.reset_index(drop = True)\n",
    "    # Adding together the added features to one dataframe.\n",
    "    y_BIG = pd.concat([y, y_shifted])\n",
    "    X.reset_index(drop = True)\n",
    "\n",
    "    # Making sure that the two dataframes match in length.\n",
    "    y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "    # Get our desired output\n",
    "    y = y_BIG[\"pv_measurement\"]\n",
    "   \n",
    "    \n",
    "    if feedback:\n",
    "        # Removing datetime object column.\n",
    "        y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "        y_features = y_features.drop('time', axis=1)\n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        X = X.reset_index(drop = True)\n",
    "        X = pd.concat([X, y_features], axis = 1)\n",
    "        \n",
    "    \n",
    "    # Removing datetime object column\n",
    "    X = X.drop(timestamps, axis=1)\n",
    "    \n",
    "    X = X.reset_index(drop = True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def pred_data_processing(X_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that reads\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X_pred = X_pred.interpolate(method = 'linear')\n",
    "    X_pred = X_pred.bfill()\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "\n",
    "    # Removing date-time from measurements\n",
    "    measurements = list(X_pred.columns.values)\n",
    "    measurements.remove(\"date_forecast\")\n",
    "    measurements.remove(\"date_calc\")\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X_pred, timestamps, measurements)\n",
    "    der_df = difference_df(X_pred, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "    l_df = log_df(X_pred, measurements)\n",
    "    dotted_df = dot_df(X_pred, 'direct_rad:W', measurements)\n",
    "    time_df = time_data_from_df(X_pred, timestamps)\n",
    "\n",
    "    X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, l_df, dotted_df, time_df], axis = \"columns\")\n",
    "\n",
    "    X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "    return X_pred_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_and_pred_data(file_paths: list[str]):\n",
    "    buildings = ['A', 'B', 'C']\n",
    "    paths = []\n",
    "    \n",
    "    for i, path in enumerate(file_paths):\n",
    "        # Retrieve data        \n",
    "        y = pd.read_parquet(path[0])\n",
    "        X_estimated = pd.read_parquet(path[1])\n",
    "        X_observed = pd.read_parquet(path[2])\n",
    "        X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "        # Processing and cleaning data\n",
    "        y = y.dropna()\n",
    "        X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "        X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "        \n",
    "        X, y= train_data_processing(X, y)\n",
    "        \n",
    "\n",
    "\n",
    "        X_pred = pred_data_processing(X_pred)\n",
    "        \n",
    "        X_path = buildings[i] + \"/\" + \"X.csv\"\n",
    "        X.to_csv(path_or_buf = X_path, sep='\\t')\n",
    "\n",
    "        y_path = buildings[i] + \"/\" + \"y.csv\"\n",
    "        y.to_csv(path_or_buf = y_path, sep='\\t')\n",
    "\n",
    "        X_pred_path = buildings[i] + \"/\" + \"X_pred.csv\"\n",
    "        X_pred.to_csv(path_or_buf = X_pred_path, sep='\\t')\n",
    "        \n",
    "        \n",
    "        paths.append([X_path, y_path, X_pred_path])\n",
    "\n",
    "    return paths \n",
    "\n",
    "def n_smallest(df: pd.DataFrame, column: str, N: int = 10) -> list[float]:\n",
    "    # Assuming df is your DataFrame and column_name is the column you're interested in\n",
    "    filtered_df = df[df[column] >= 10]\n",
    "\n",
    "    # Replace N with the number of lowest non-zero values you want\n",
    "    \n",
    "    result = filtered_df.nsmallest(N, column)\n",
    "    return result\n",
    "\n",
    "def n_highest(df: pd.DataFrame, column: str, N: int = 10) -> list[float]:\n",
    "    # Replace N with the number of largest non-zero values you want\n",
    "    N = 10\n",
    "    result = df.nlargest(N, column)\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "#               ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "#               ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "# file_paths = create_training_and_pred_data(file_paths = file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Value of Day\n",
    "Predicts when a max value occurs during a day. This to put it simply did not work. It had incredibly poor predictions for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a max indication, and may be used to generate additional features\n",
    "def max_day_df(y: pd.DataFrame)-> pd.DataFrame:\n",
    "    y = y.copy()\n",
    "    df = pd.DataFrame()\n",
    "    # Group by day and find the max measurement for each day\n",
    "    y['date'] = y['time'].dt.date \n",
    "    df['pv_measurement']  = np.where(y['pv_measurement'] == y.groupby('date')['pv_measurement'].transform('max'), 1, 0)\n",
    "    df['time'] = y['time']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], add_feedback: bool = False):\n",
    "   \n",
    "#     # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "#     X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "\n",
    "#     # Extract necesarry values for feature generation.\n",
    "#     timestamps = \"date_forecast\"\n",
    "#     measurements = list(X.columns.values)\n",
    "#     measurements.remove(timestamps)\n",
    "\n",
    "#     # Probable features that may be used\n",
    "#     squared_df = square_df(X, timestamps, measurements)\n",
    "#     der_df = difference_df(X, timestamps, measurements)\n",
    "#     dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "#     int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "#     dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "#     time_df = time_data_from_df(X, timestamps)\n",
    "\n",
    "#     X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "#     if len(filter_list) > 0:\n",
    "#         X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "#     # Additional features\n",
    "#     der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "#     der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "#     y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "\n",
    "#     # Adding together the added features to one dataframe.\n",
    "#     y_BIG = pd.concat([y, der_y_shifted, y_shifted])\n",
    "\n",
    "\n",
    "#     # Making sure that the two dataframes match in length.\n",
    "#     y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "#     # Get our desired output\n",
    "#     y = y_BIG[\"pv_measurement\"]\n",
    "#     y = y.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "#     if add_feedback:\n",
    "#         # Removing datetime object column.\n",
    "#         y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "#         y_features = y_features.drop('time', axis=1)\n",
    "#         y_features = y_features.reset_index(drop = True)\n",
    "\n",
    "#         X = pd.concat([X, y_features], axis = 1)\n",
    "    \n",
    "#     # Removing datetime object column\n",
    "#     X = X.drop(timestamps, axis=1)\n",
    "#     X = X.reset_index(drop = True)\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# def pred_data_processing(X_pred: pd.DataFrame, filter_list: list[str] = []) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     A function that reads\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "#     X_pred = X_pred.interpolate(method = 'linear')\n",
    "#     X_pred = X_pred.bfill()\n",
    "\n",
    "#     # Extract necesarry values for feature generation.\n",
    "#     timestamps = \"date_forecast\"\n",
    "\n",
    "#     # Removing date-time from measurements\n",
    "#     measurements = list(X_pred.columns.values)\n",
    "#     measurements.remove(\"date_forecast\")\n",
    "#     measurements.remove(\"date_calc\")\n",
    "\n",
    "#     # Probable features that may be used\n",
    "#     squared_df = square_df(X_pred, timestamps, measurements)\n",
    "#     der_df = difference_df(X_pred, timestamps, measurements)\n",
    "#     dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "#     int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "#     dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "#     time_df = time_data_from_df(X_pred, timestamps)\n",
    "\n",
    "    \n",
    "\n",
    "#     X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "#     if len(filter_list) > 0:\n",
    "#         X_pred_new = X_pred_new[filter_list + ['date_forecast']]\n",
    "#     else:\n",
    "#         X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "\n",
    "#     return X_pred_new\n",
    "\n",
    "# class feature_learner:\n",
    "#     def __init__(self, file_paths: list[list[str]], features: list[str] = [], save_folder: str = \"\") -> None:\n",
    "#         self.file_paths = file_paths\n",
    "#         self.features = features\n",
    "#         self.save_folder = save_folder + \"/\"\n",
    "#         self.buildings = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "#     def create_training_data_multi_model(self):\n",
    "#         self.X_train_sets = []\n",
    "#         self.X_test_sets = []\n",
    "#         self.y_train_sets = []\n",
    "#         self.y_test_sets = []\n",
    "#         self.X_pred_sets = []\n",
    "\n",
    "#         for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "#             y = pd.read_parquet(path[0])\n",
    "#             X_estimated = pd.read_parquet(path[1])\n",
    "#             X_observed = pd.read_parquet(path[2])\n",
    "#             X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "#             # =================  TEST DATA  ================\n",
    "#             X_pred = pred_data_processing(X_pred, self.features)\n",
    "#             X_pred['building'] = i\n",
    "\n",
    "#             # =================TRAINING DATA================\n",
    "#             # Pre-process data\n",
    "#             y = y.dropna()\n",
    "#             X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "#             X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "#             y = max_day_df(y)\n",
    "\n",
    "#             # ================= ENSURING DATA LENGTH MATCHES ==================\n",
    "#             X, y= train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "#             # ================= ADD BUILDING FEATURE ================\n",
    "#             X['building'] = i\n",
    "\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "\n",
    "#             # ================= SAVING ALL SETS ======================\n",
    "#             self.X_train_sets.append(X_train)\n",
    "#             self.X_test_sets.append(X_test)\n",
    "#             self.y_train_sets.append(y_train)\n",
    "#             self.y_test_sets.append(y_test)\n",
    "#             self.X_pred_sets.append(X_pred)\n",
    "        \n",
    "#     def fit_multi_model(self):\n",
    "        \n",
    "#         self.models = []\n",
    "\n",
    "#         for i in range(len(self.X_train_sets)): \n",
    "            \n",
    "#             train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "\n",
    "#             model = cb.CatBoostClassifier(loss_function=\"Logloss\", logging_level='Silent')\n",
    "\n",
    "#             grid = {'iterations': [100, 150, 200],\n",
    "#                     'learning_rate': [0.03, 0.1],\n",
    "#                     'depth': [2, 4, 6, 8],\n",
    "#                     'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "#             model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "#             self.models.append(model)\n",
    "\n",
    "#     def predict_multi_model(self):\n",
    "#         preds = []\n",
    "        \n",
    "#         for i in range(len(self.X_pred_sets)):\n",
    "#             X_pred = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "#             unformated_pred = self.models[i].predict(X_pred)\n",
    "            \n",
    "#             unformated_pred_df = pd.DataFrame()\n",
    "#             unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "#             unformated_pred_df[\"building\"] = self.X_pred_sets[i][\"building\"]\n",
    "\n",
    "#             replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "#             # Use the replace method with the specified column and dictionary\n",
    "#             unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "#             unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            \n",
    "#             unformated_pred_df.to_csv(self.buildings[i] + \"max_pred\" + \".csv\")\n",
    "\n",
    "#     def get_performance_multi_model(self) -> None:\n",
    "        \n",
    "#         for i in range(len(self.X_test_sets)):\n",
    "#             pred = self.models[i].predict(self.X_test_sets[i])\n",
    "#             accuracy = accuracy_score(self.y_test_sets[i], pred)\n",
    "#             print(\"SCORE BUILDING \" + self.buildings[i])\n",
    "#             print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "#               ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "#               ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "\n",
    "# features = [\"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\", \"ceiling_height_agl:m\", \"clear_sky_energy_1h:J\", \n",
    "#            \"clear_sky_rad:W\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\", \n",
    "#            \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"msl_pressure:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "#            \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", 'day_of_year:day', 'month:month', 'hour:hour']\n",
    "\n",
    "\n",
    "# # Testing the procedure \n",
    "# l = feature_learner(file_paths = file_paths, features = features)\n",
    "# l.create_training_data_multi_model()\n",
    "# l.fit_multi_model()\n",
    "# l.get_performance_multi_model()\n",
    "# l.predict_multi_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "Testing different Machine Learning models, and tuning and adjusting features quickly becomes a bother if one does not create helper function and classes. The code bellow is what the Project Group developed during the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [[\"A/X.csv\", \"A/y.csv\", \"A/X_pred.csv\"],\n",
    "              [\"B/X.csv\", \"B/y.csv\", \"B/X_pred.csv\"],\n",
    "              [\"C/X.csv\", \"C/y.csv\", \"C/X_pred.csv\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to organize the different steps in the machine learning pipeline. The class contains some nice helper functions\n",
    "that helps the user gain insight into what features the model finds the most usefull.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class learner:\n",
    "    def __init__(self, file_paths: list[list[str]], features: list[str] = [], save_folder: str = \"\", feedback: bool = False) -> None:\n",
    "        self.file_paths = file_paths\n",
    "        self.features = features\n",
    "        self.save_folder = save_folder + \"/\"\n",
    "        self.buildings = [\"A\", \"B\", \"C\"]\n",
    "        self.feedback = feedback\n",
    "\n",
    "    def create_training_data_multi_model(self):\n",
    "        self.X_train_sets = []\n",
    "        self.X_test_sets = []\n",
    "        self.y_train_sets = []\n",
    "        self.y_test_sets = []\n",
    "        self.X_pred_sets = []\n",
    "        self.y_sets = []\n",
    "\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            X = pd.read_csv(path[0], sep='\\t')[self.features]\n",
    "            \n",
    "            y = pd.read_csv(path[1], sep='\\t')['pv_measurement']\n",
    "\n",
    "            X_pred = pd.read_csv(path[2], sep='\\t')[self.features + [\"date_forecast\"]]\n",
    "            X_pred[\"date_forecast\"] = pd.to_datetime(X_pred[\"date_forecast\"])\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "            print(y)\n",
    "\n",
    "            # ================= SAVING ALL SETS ======================\n",
    "            self.X_train_sets.append(X_train)\n",
    "            self.X_test_sets.append(X_test)\n",
    "            self.y_train_sets.append(y_train)\n",
    "            self.y_test_sets.append(y_test)\n",
    "            self.X_pred_sets.append(X_pred)\n",
    "            self.y_sets.append(y)\n",
    "            \n",
    "\n",
    "\n",
    "    def create_training_data(self):\n",
    "        list_y = []\n",
    "        list_X = []\n",
    "        list_X_pred = []\n",
    "        scalers = []\n",
    "    \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            y = pd.read_parquet(path[0])\n",
    "            X_estimated = pd.read_parquet(path[1])\n",
    "            X_observed = pd.read_parquet(path[2])\n",
    "            X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred = dp.pred_data_processing(X_pred, self.features, self.feedback)\n",
    "            X_pred['building'] = i\n",
    "            \n",
    "            list_X_pred.append(X_pred)\n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            # Pre-process data\n",
    "            y = y.dropna()\n",
    "            X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "            X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "            # BETTER NAME\n",
    "            X, y= dp.train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = i\n",
    "\n",
    "            list_y.append(y)\n",
    "            list_X.append(X)\n",
    "\n",
    "        self.scalers = scalers\n",
    "        # Add all the lists together. However there is a need to add set\n",
    "        y = pd.concat(list_y, axis= 0, ignore_index=True)\n",
    "        X = pd.concat(list_X, axis= 0, ignore_index=True)\n",
    "        X_pred = pd.concat(list_X_pred, axis = 0, ignore_index=True)\n",
    "        \n",
    "        X = X.reset_index(drop=True)\n",
    "        X_pred = X_pred.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "        \n",
    "        # Should try scaling later ... :/\n",
    "        self.X_train, self.X_test, self.X_pred = X_train, X_test, X_pred\n",
    "    \n",
    "    def fit_multi_model_h2o(self):\n",
    "        \n",
    "        h2o.init()\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(len(self.X_train_sets)):\n",
    "            train_frame =  pd.concat([self.X_train_sets[i], self.y_train_sets[i]], axis = \"columns\")\n",
    "            h2o_frame = h2o.H2OFrame(train_frame)\n",
    "\n",
    "            x_train_columns = h2o_frame.columns\n",
    "\n",
    "            model = H2OAutoML(sort_metric='MAE', max_models=10, exclude_algos=[\"DeepLearning\"])\n",
    "\n",
    "            model.train(x = x_train_columns, y = \"pv_measurement\", training_frame=h2o_frame)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def fit_multi_model(self):\n",
    "        \n",
    "        self.models = []\n",
    "\n",
    "        for i in range(len(self.X_train_sets)): \n",
    "            \n",
    "            train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "\n",
    "            model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "            grid = {'iterations': [100, 150, 200],\n",
    "                    'learning_rate': [0.03, 0.1],\n",
    "                    'depth': [2, 4, 6, 8],\n",
    "                    'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "            model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "            self.models.append(model)\n",
    "        return self.models\n",
    "\n",
    "\n",
    "    def get_performance_multi_model_h2o(self):\n",
    "        mae_sum = 0\n",
    "\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            h2o_frame = h2o.H2OFrame(self.X_test_sets[i])\n",
    "\n",
    "            pred_h2o_frame = self.models[i].predict(h2o_frame)\n",
    "            pred_df = pred_h2o_frame.as_data_frame()\n",
    "            pred = pred_df['predict'].to_list()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], np.array(pred)))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"MAE: \", mae_sum)\n",
    "\n",
    "    def predict_multi_model_h2o(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            h2o_frame = h2o.H2OFrame(self.X_pred_sets[i])\n",
    "\n",
    "            pred_h2o_frame = self.models[i].predict(h2o_frame)\n",
    "            pred_df = pred_h2o_frame.as_data_frame()\n",
    "            X_pred = pred_df['predict'].to_list()\n",
    "            \n",
    "            unformated_pred = np.array(X_pred)\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.buildings[i]\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(np.array(unformated_pred))\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost 😕\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def predict_multi_model(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            if self.feedback:\n",
    "                X_pred_without_feedback = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "                pred_list = []\n",
    "                # First feedback\n",
    "\n",
    "                \n",
    "                y_df = self.y_sets[i].to_frame()\n",
    "\n",
    "                feedback_init_row = shifted_values_24_h(y_df, \"pv_measurement\").iloc[-1]\n",
    "                \n",
    "                feedback_row = feedback_init_row\n",
    "\n",
    "                \n",
    "\n",
    "                for j, idx in enumerate(X_pred_without_feedback.index.to_list()):\n",
    "                   \n",
    "                    X_pred_with_feedback = pd.concat([X_pred_without_feedback.loc[idx].reset_index(drop=True), feedback_row.reset_index(drop=True)], axis=0, ignore_index=True)\n",
    "                    \n",
    "                    feedback = self.models[i].predict(X_pred_with_feedback)\n",
    "                    \n",
    "                    # Shift the values to the right\n",
    "                    feedback_row.iloc[1:] = feedback_row.iloc[:-1].values\n",
    "                    \n",
    "                    feedback_row.iloc[0] = feedback\n",
    "                    pred_list.append(feedback)\n",
    "\n",
    "                unformated_pred = np.array(pred_list)\n",
    "                \n",
    "                series = pd.Series(unformated_pred)\n",
    "                series.plot()\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                X_pred = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "                unformated_pred = self.models[i].predict(X_pred)\n",
    "\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.buildings[i]\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(10**np.array(unformated_pred))\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def get_performance_multi_model(self) -> None:\n",
    "        mae_sum = 0\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            pred = self.models[i].predict(self.X_test_sets[i])\n",
    "            pd.Series(pred).plot()\n",
    "            pd.Series(self.y_test_sets[i]).plot()\n",
    "            plt.show()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], 10**np.array(pred)))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"Mean absolute error: \", mae_sum/len(self.X_test_sets))\n",
    "\n",
    "\n",
    "\n",
    "    def fit_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Based on the selected model the class switches between what model is doing the learning. \n",
    "        \"\"\"\n",
    "\n",
    "        #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "        # Add a function that picks between different models, and processes the data based on this\n",
    "        train_dataset = cb.Pool(self.X_train, self.y_train)\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "        grid = {'iterations': [100, 150, 200],\n",
    "                'learning_rate': [0.03, 0.1],\n",
    "                'depth': [2, 4, 6, 8],\n",
    "                'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "        self.model.grid_search(grid, train_dataset, verbose=False)\n",
    "        \n",
    "\n",
    "    def get_performance(self) -> None:\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        pd.Series(pred).plot()\n",
    "        pd.Series(self.y_test).plot()\n",
    "        \n",
    "        mae = (mean_absolute_error(self.y_test, pred))\n",
    "        print(\"Mean Abs: {:.2f}\".format(mae))\n",
    "\n",
    "    def predict(self) -> None:\n",
    "        \n",
    "        X_pred = self.X_pred.drop('date_forecast', axis = 1)\n",
    "        unformated_pred = self.model.predict(X_pred)\n",
    "        \n",
    "        unformated_pred_df = pd.DataFrame()\n",
    "        unformated_pred_df[\"date_forecast\"] = self.X_pred[\"date_forecast\"]\n",
    "        unformated_pred_df[\"building\"] = self.X_pred[\"building\"]\n",
    "\n",
    "        replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "        # Use the replace method with the specified column and dictionary\n",
    "        unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "        unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "        unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "    \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def save_best_features(self, filename: str, N: int = 0):\n",
    "        if N == 0:\n",
    "            N = len(self.X_test_sets[0].columns.values) - 1\n",
    "        best_features_df = pd.DataFrame()\n",
    "\n",
    "        for i, X in enumerate(self.X_test_sets):\n",
    "           \n",
    "            feature_importance = self.models[i].get_feature_importance()\n",
    "\n",
    "            # Pair feature names with their importance scores\n",
    "            feature_importance_dict = dict(zip(self.models[i].feature_names_, feature_importance))\n",
    "\n",
    "            # Sort features by importance\n",
    "            sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Print or use the top features\n",
    "            top_features = sorted_feature_importance[:N]  # Replace N with the number of top features you want\n",
    "            \n",
    "            # Saving to list\n",
    "            labels = list(X.columns.values)\n",
    "            best_features = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                best_features.append(feat[0])\n",
    "            \n",
    "            best_features_df[\"Model \" + self.buildings[i]] = pd.Series(np.array(best_features))\n",
    "\n",
    "        best_features_df.to_csv(\"tests/\" + self.save_folder + filename + '.csv', sep ='\\t')\n",
    "\n",
    "\n",
    "    def _format_predictions(self, unformated_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # \n",
    "        to_be_submitted_index = pd.read_csv(\"test.csv\")\n",
    "\n",
    "        #convert the \"time\" column to datetime\n",
    "        to_be_submitted_index[\"time\"] = pd.to_datetime(to_be_submitted_index[\"time\"])\n",
    "        pred = pd.merge(unformated_pred, to_be_submitted_index, how='inner', left_on=['date_forecast', 'building'], right_on=[\"time\", \"location\"])\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "        \n",
    "    def _save_predictions(self, pred: pd.DataFrame)->None:\n",
    "        #Make the index and pv_measurement column into a csv file\n",
    "        pred[[\"id\", \"pv_measurement\"]].rename(columns={\"id\" : \"id\" , \"pv_measurement\" : \"prediction\"}).to_csv(\"tests/\" + self.save_folder + \"model_pred.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the user may apply their testing setup. Here they may apply what features they need, and the model will ensure the rest.\n",
    "\n",
    "TODO: Dot signals\n",
    "\n",
    "TODO: Add log of signals\n",
    "\n",
    "TODO: Sin of signals and dot them :/\n",
    "\n",
    "TODO: Test adding the best features.\n",
    "\n",
    "IDEA: The directions may be helpfull. Create a number that tells us more about the sun position. Break it down into breakpoints between them /:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.00\n",
      "1         0.00\n",
      "2         0.00\n",
      "3         0.00\n",
      "4        19.36\n",
      "         ...  \n",
      "34056     9.02\n",
      "34057     0.00\n",
      "34058     0.00\n",
      "34059     0.00\n",
      "34060     0.00\n",
      "Name: pv_measurement, Length: 34061, dtype: float64\n",
      "0        0.000000\n",
      "1        0.000000\n",
      "2        0.000000\n",
      "3        0.000000\n",
      "4        0.000000\n",
      "           ...   \n",
      "32814    0.828587\n",
      "32815   -0.000000\n",
      "32816   -0.000000\n",
      "32817   -0.000000\n",
      "32818   -0.000000\n",
      "Name: pv_measurement, Length: 32819, dtype: float64\n",
      "0        137.20\n",
      "1          0.00\n",
      "2          0.00\n",
      "3          0.00\n",
      "4          0.00\n",
      "          ...  \n",
      "26066     50.96\n",
      "26067      2.94\n",
      "26068      0.00\n",
      "26069     -0.00\n",
      "26070     -0.00\n",
      "Name: pv_measurement, Length: 26071, dtype: float64\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-7.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-7 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-7 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-7 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table th,\n",
       "#h2o-table-7 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>2 hours 19 mins</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.44.0.1</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>22 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_asber_r72973</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>2.247 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.12 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         2 hours 19 mins\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.44.0.1\n",
       "H2O_cluster_version_age:    22 days\n",
       "H2O_cluster_name:           H2O_from_python_asber_r72973\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    2.247 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.12 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "18:04:54.680: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "██████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "18:09:11.222: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "██████████████████████████████████████████████████████████████| (done) 100%\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |█\n",
      "18:13:12.557: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "████████████████"
     ]
    }
   ],
   "source": [
    "# features = ['direct_rad:W', 'is_in_shadow:idx', 'total_cloud_cover:p', 'sun_elevation:d', \n",
    "#             'derivative_direct_rad:W_d', 'derivative_sun_elevation:d_d', \n",
    "#             'sun_azimuth:d', 't_1000hPa:K', \n",
    "#             'derivative_sun_azimuth:d_d', 'derivative_t_1000hPa:K_d',\n",
    "#             'day_of_year:day', 'month:month', 'hour:hour']\n",
    "\n",
    "# \n",
    "\n",
    "features = [\"clear_sky_energy_1h:J\", \"clear_sky_rad:W\", \"cloud_base_agl:m\", \"dew_or_rime:idx\", \"dew_point_2m:K\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\",\n",
    "           \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"is_in_shadow:idx\", \"msl_pressure:hPa\",\n",
    "           \"precip_5min:mm\", \"pressure_100m:hPa\", \"pressure_50m:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "           \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", \"visibility:m\",\n",
    "           \"wind_speed_10m:ms\"]\n",
    "\n",
    "\n",
    "# Should add features etc, but a bit stranve\n",
    "\n",
    "# Testing the procedure \n",
    "l = learner(file_paths = file_paths, features = features)\n",
    "l.create_training_data_multi_model()\n",
    "l.fit_multi_model_h2o()\n",
    "l.get_performance_multi_model_h2o()\n",
    "l.predict_multi_model_h2o()\n",
    "\n",
    "# IDEA: Add a different scaler for all the different features, then transform then into the right scale after the fact\n",
    "# Remove any needless features, add until you get a good outpu\n",
    "# Probable reason for the bad score is the scaling defeciencies.\n",
    "# Get the smallest value that is NoN zero, and cap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
