{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173 - Machine Learning Group - Project\n",
    "This is an explanatory Jupyter Notebook that is intended to show how the Project Group approached the \"Solar Energy Production Forecasting\" challenge. \n",
    "\n",
    "### Table of Contents:\n",
    " 1. [Exploratory Data Analysis (EDA)](#Exploratory-Data-Analysis-(EDA)) \n",
    " 2. [Feature Extraction](#Feature-Extraction) \n",
    " 3. [Machine Learning Pipeline](#Machine-Learning-Pipeline) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data Processing Tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "import catboost as cb\n",
    "import h2o \n",
    "from h2o.automl import H2OAutoML \n",
    "\n",
    "\n",
    "# Machine Learning Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_length_matching(train: pd.DataFrame, obs: pd.DataFrame)-> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function is intended to ensure that both the training data and\n",
    "    the observed data are sorted, and contain the same number of entries. \n",
    "    \"\"\"\n",
    "\n",
    "    # Cut the data frames so that their date match.\n",
    "    obs_feature_test = obs[obs['date_forecast'].isin(train['time'])].sort_values(by=['date_forecast'])  # sortert etter datao\n",
    "    # If only one of them has the date ensure that the other also has the same sorting.\n",
    "    train_feature_test = train[train['time'].isin(obs['date_forecast'])].sort_values(by=['time'])       # sortert etter datao\n",
    "\n",
    "    return train_feature_test, obs_feature_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]):\n",
    "    squared_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        squared_df['squared_' + measurement + '_2'] = df[measurement]**2\n",
    "    return squared_df\n",
    "\n",
    "def upscale_(df: pd.DataFrame, feature: str, upscale: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    upscale_df = pd.DataFrame()\n",
    "    \n",
    "    upscale_df[\"uscale_\" + feature] = df[feature]*feature\n",
    "\n",
    "    return upscale_df\n",
    "\n",
    "def dot_df(df: pd.DataFrame, dot_feature: str, features: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dot_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        dot_df[dot_feature + '_dot_' + feature] = df[dot_feature] * df[feature]\n",
    "\n",
    "    return dot_df\n",
    "\n",
    "def log_df(df: pd.DataFrame, features: list[str]):\n",
    "    \n",
    "    df = df.copy()\n",
    "    log_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        log_df['log_' + feature] =  np.log(df[feature] + 1)\n",
    "\n",
    "    return log_df\n",
    "\n",
    "\n",
    "def difference_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    der_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        der_df['derivative_' + measurement + '_d'] = df[measurement].diff()\n",
    "    \n",
    "    df = df.drop('time_diff', axis =  1)\n",
    "\n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    der_df = der_df.interpolate(method='linear')\n",
    "    der_df = der_df.bfill()\n",
    "    \n",
    "    return der_df\n",
    "\n",
    "def double_derivative_from_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    dder_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    # Calculate derivative estimates\n",
    "    for measurement in measurements:\n",
    "        dder_df['double_derivative_' + measurement + '_dd'] = df[measurement].diff() / (divmod(df['time_diff'].dt.total_seconds(), 60)[0]**2)\n",
    "    \n",
    "    df = df.drop('time_diff', axis=1)\n",
    "    \n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    dder_df = dder_df.interpolate(method='linear')\n",
    "    dder_df = dder_df.bfill()\n",
    "\n",
    "    return dder_df\n",
    "\n",
    "def daily_accumulated_val_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    i_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        i_df['integral_' + measurement + '_integral'] = df.groupby('date')[measurement].cumsum()\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return i_df\n",
    "\n",
    "def daily_accumulated_val_squared_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    di_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        di_df['double_integral_' + measurement + '_dintegral'] = df.groupby('date')[measurement].cumsum()**2\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return di_df\n",
    "\n",
    "def time_data_from_df(df: pd.DataFrame, timestamps: str) -> pd.DataFrame: \n",
    "    # Extracting components\n",
    "    time_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    time_df['day_of_year:day'] = df[timestamps].dt.dayofyear\n",
    "    time_df['month:month'] = df[timestamps].dt.month\n",
    "    #time_df['year:year'] = df[timestamps].dt.year\n",
    "    time_df['hour:hour'] = df[timestamps].dt.hour\n",
    "    return time_df\n",
    "\n",
    "\n",
    "\n",
    "def n_largest_freq(df: pd.DataFrame, measurements: list[str],n_largest: int):\n",
    "    \"\"\"\n",
    "    Generates values based on the largest frequencies that are present.\n",
    "    \"\"\"\n",
    "    for measurement in measurements:\n",
    "        signal = df[measurement].values\n",
    "\n",
    "        fft_result = np.fft.fft(signal)\n",
    "        \n",
    "        # Keep only the dominant frequencies (e.g., top 5)\n",
    "        num_components_to_keep = n_largest\n",
    "\n",
    "        indices = np.argsort(np.abs(fft_result))[::-1][:num_components_to_keep]\n",
    "\n",
    "        # Set all other frequency components to zero\n",
    "        fft_result_filtered = np.zeros_like(fft_result)\n",
    "        fft_result_filtered[indices] = fft_result[indices]\n",
    "\n",
    "        # Compute IFFT\n",
    "        ifft_result = np.fft.ifft(fft_result_filtered)\n",
    "\n",
    "        # Add the filtered results to the dataframe\n",
    "        df[\"filtered_\" + measurement] = ifft_result.real\n",
    "\n",
    "    return df\n",
    "\n",
    "def freq_comb(df: pd.DataFrame, features: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Takes the fourier transform of multiple signals add them together, and then takes the inverse.\n",
    "\n",
    "    features: Are what you would like to combine.\n",
    "    df: Chosen dataframe containing feature information.\n",
    "    \"\"\"\n",
    "\n",
    "    total_fft = 0\n",
    "    \n",
    "    for feat in features:\n",
    "        # Finding the signal directly might be wrong due to timestamps and such, but might still be helpful. It is not correct, but improvements like day by day sampling might be useful.\n",
    "        signal = df[feat].values\n",
    "\n",
    "        # Min-max scaling\n",
    "        scaled_signal = min_max_scale(signal)\n",
    "        \n",
    "        fft = np.fft.fft(scaled_signal)\n",
    "        total_fft = total_fft + fft\n",
    "    \n",
    "    ifft_result = np.fft.ifft(total_fft)\n",
    "\n",
    "    return ifft_result.real\n",
    "\n",
    "def min_max_scale(signal: np.array) -> np.array:\n",
    "    # Calculate min and max values\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "\n",
    "    # Min-max scaling\n",
    "    scaled_signal = (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_signal\n",
    "\n",
    "def shifted_values_24_h(y: pd.DataFrame, measurement: str)->pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1, 25):\n",
    "        df[measurement + 'n-' + str(i)] = y[measurement].shift(i)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(df: pd.DataFrame):\n",
    "    # Extract the part before \":\" in column names\n",
    "    df.columns = df.columns.str.split(':').str[0]\n",
    "\n",
    "    # Group by modified column names and sum values\n",
    "    grouped_df = df.groupby(df.columns, axis=1).sum()\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A couple of functions to generate the approprate features for both training and prediction data.\n",
    "\"\"\"\n",
    "\n",
    "def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], feedback: bool = False):\n",
    "   \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "    measurements = list(X.columns.values)\n",
    "    measurements.remove(timestamps)\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X, timestamps, measurements)\n",
    "    der_df = difference_df(X, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "    l_df = log_df(X, measurements)\n",
    "    time_df = time_data_from_df(X, timestamps)\n",
    "\n",
    "    X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, l_df, time_df], axis = \"columns\")\n",
    "\n",
    "    if len(filter_list) > 0:\n",
    "        X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "    # Additional features\n",
    "    \n",
    "    #der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "    # der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "    y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "    y.reset_index(drop = True)\n",
    "    y_shifted.reset_index(drop = True)\n",
    "    # Adding together the added features to one dataframe.\n",
    "    y_BIG = pd.concat([y, y_shifted])\n",
    "    X.reset_index(drop = True)\n",
    "\n",
    "    # Making sure that the two dataframes match in length.\n",
    "    y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "    # Get our desired output\n",
    "    y = y_BIG[\"pv_measurement\"]\n",
    "   \n",
    "    \n",
    "    if feedback:\n",
    "        # Removing datetime object column.\n",
    "        y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "        y_features = y_features.drop('time', axis=1)\n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        X = X.reset_index(drop = True)\n",
    "        X = pd.concat([X, y_features], axis = 1)\n",
    "        \n",
    "    \n",
    "    # Removing datetime object column\n",
    "    X = X.drop(timestamps, axis=1)\n",
    "    \n",
    "    X = X.reset_index(drop = True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def pred_data_processing(X_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that reads\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X_pred = X_pred.interpolate(method = 'linear')\n",
    "    X_pred = X_pred.bfill()\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "\n",
    "    # Removing date-time from measurements\n",
    "    measurements = list(X_pred.columns.values)\n",
    "    measurements.remove(\"date_forecast\")\n",
    "    measurements.remove(\"date_calc\")\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X_pred, timestamps, measurements)\n",
    "    der_df = difference_df(X_pred, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "    l_df = log_df(X_pred, measurements)\n",
    "    time_df = time_data_from_df(X_pred, timestamps)\n",
    "\n",
    "    X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, l_df, time_df], axis = \"columns\")\n",
    "\n",
    "    X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "    return X_pred_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_and_pred_data(file_paths: list[str]):\n",
    "    buildings = ['A', 'B', 'C']\n",
    "    paths = []\n",
    "    \n",
    "    for i, path in enumerate(file_paths):\n",
    "        # Retrieve data        \n",
    "        y = pd.read_parquet(path[0])\n",
    "        X_estimated = pd.read_parquet(path[1])\n",
    "        X_observed = pd.read_parquet(path[2])\n",
    "        X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "        # Processing and cleaning data\n",
    "        y = y.dropna()\n",
    "        X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "        X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "        \n",
    "        X, y= train_data_processing(X, y)\n",
    "        \n",
    "\n",
    "\n",
    "        X_pred = pred_data_processing(X_pred)\n",
    "        \n",
    "        X_path = buildings[i] + \"/\" + \"X.csv\"\n",
    "        X.to_csv(path_or_buf = X_path, sep='\\t')\n",
    "\n",
    "        y_path = buildings[i] + \"/\" + \"y.csv\"\n",
    "        y.to_csv(path_or_buf = y_path, sep='\\t')\n",
    "\n",
    "        X_pred_path = buildings[i] + \"/\" + \"X_pred.csv\"\n",
    "        X_pred.to_csv(path_or_buf = X_pred_path, sep='\\t')\n",
    "        \n",
    "        \n",
    "        paths.append([X_path, y_path, X_pred_path])\n",
    "\n",
    "    return paths \n",
    "\n",
    "def n_smallest(df: pd.DataFrame, column: str, N: int = 10) -> list[float]:\n",
    "    # Assuming df is your DataFrame and column_name is the column you're interested in\n",
    "    filtered_df = df[df[column] >= 10]\n",
    "\n",
    "    # Replace N with the number of lowest non-zero values you want\n",
    "    \n",
    "    result = filtered_df.nsmallest(N, column)\n",
    "    return result\n",
    "\n",
    "def n_highest(df: pd.DataFrame, column: str, N: int = 10) -> list[float]:\n",
    "    # Replace N with the number of largest non-zero values you want\n",
    "    N = 10\n",
    "    result = df.nlargest(N, column)\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "c:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\pandas\\core\\arraylike.py:396: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "              ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "              ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "file_paths = create_training_and_pred_data(file_paths = file_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Value of Day\n",
    "Predicts when a max value occurs during a day. This to put it simply did not work. It had incredibly poor predictions for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a max indication, and may be used to generate additional features\n",
    "def max_day_df(y: pd.DataFrame)-> pd.DataFrame:\n",
    "    y = y.copy()\n",
    "    df = pd.DataFrame()\n",
    "    # Group by day and find the max measurement for each day\n",
    "    y['date'] = y['time'].dt.date \n",
    "    df['pv_measurement']  = np.where(y['pv_measurement'] == y.groupby('date')['pv_measurement'].transform('max'), 1, 0)\n",
    "    df['time'] = y['time']\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], add_feedback: bool = False):\n",
    "   \n",
    "#     # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "#     X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "\n",
    "#     # Extract necesarry values for feature generation.\n",
    "#     timestamps = \"date_forecast\"\n",
    "#     measurements = list(X.columns.values)\n",
    "#     measurements.remove(timestamps)\n",
    "\n",
    "#     # Probable features that may be used\n",
    "#     squared_df = square_df(X, timestamps, measurements)\n",
    "#     der_df = difference_df(X, timestamps, measurements)\n",
    "#     dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "#     int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "#     dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "#     time_df = time_data_from_df(X, timestamps)\n",
    "\n",
    "#     X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "#     if len(filter_list) > 0:\n",
    "#         X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "#     # Additional features\n",
    "#     der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "#     der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "#     y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "\n",
    "#     # Adding together the added features to one dataframe.\n",
    "#     y_BIG = pd.concat([y, der_y_shifted, y_shifted])\n",
    "\n",
    "\n",
    "#     # Making sure that the two dataframes match in length.\n",
    "#     y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "#     # Get our desired output\n",
    "#     y = y_BIG[\"pv_measurement\"]\n",
    "#     y = y.reset_index(drop = True)\n",
    "    \n",
    "    \n",
    "#     if add_feedback:\n",
    "#         # Removing datetime object column.\n",
    "#         y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "#         y_features = y_features.drop('time', axis=1)\n",
    "#         y_features = y_features.reset_index(drop = True)\n",
    "\n",
    "#         X = pd.concat([X, y_features], axis = 1)\n",
    "    \n",
    "#     # Removing datetime object column\n",
    "#     X = X.drop(timestamps, axis=1)\n",
    "#     X = X.reset_index(drop = True)\n",
    "\n",
    "#     return X, y\n",
    "\n",
    "# def pred_data_processing(X_pred: pd.DataFrame, filter_list: list[str] = []) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     A function that reads\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "#     X_pred = X_pred.interpolate(method = 'linear')\n",
    "#     X_pred = X_pred.bfill()\n",
    "\n",
    "#     # Extract necesarry values for feature generation.\n",
    "#     timestamps = \"date_forecast\"\n",
    "\n",
    "#     # Removing date-time from measurements\n",
    "#     measurements = list(X_pred.columns.values)\n",
    "#     measurements.remove(\"date_forecast\")\n",
    "#     measurements.remove(\"date_calc\")\n",
    "\n",
    "#     # Probable features that may be used\n",
    "#     squared_df = square_df(X_pred, timestamps, measurements)\n",
    "#     der_df = difference_df(X_pred, timestamps, measurements)\n",
    "#     dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "#     int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "#     dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "#     time_df = time_data_from_df(X_pred, timestamps)\n",
    "\n",
    "    \n",
    "\n",
    "#     X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, time_df], axis = \"columns\")\n",
    "\n",
    "#     if len(filter_list) > 0:\n",
    "#         X_pred_new = X_pred_new[filter_list + ['date_forecast']]\n",
    "#     else:\n",
    "#         X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "\n",
    "#     return X_pred_new\n",
    "\n",
    "# class feature_learner:\n",
    "#     def __init__(self, file_paths: list[list[str]], features: list[str] = [], save_folder: str = \"\") -> None:\n",
    "#         self.file_paths = file_paths\n",
    "#         self.features = features\n",
    "#         self.save_folder = save_folder + \"/\"\n",
    "#         self.buildings = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "#     def create_training_data_multi_model(self):\n",
    "#         self.X_train_sets = []\n",
    "#         self.X_test_sets = []\n",
    "#         self.y_train_sets = []\n",
    "#         self.y_test_sets = []\n",
    "#         self.X_pred_sets = []\n",
    "\n",
    "#         for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "#             y = pd.read_parquet(path[0])\n",
    "#             X_estimated = pd.read_parquet(path[1])\n",
    "#             X_observed = pd.read_parquet(path[2])\n",
    "#             X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "#             # =================  TEST DATA  ================\n",
    "#             X_pred = pred_data_processing(X_pred, self.features)\n",
    "#             X_pred['building'] = i\n",
    "\n",
    "#             # =================TRAINING DATA================\n",
    "#             # Pre-process data\n",
    "#             y = y.dropna()\n",
    "#             X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "#             X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "#             y = max_day_df(y)\n",
    "\n",
    "#             # ================= ENSURING DATA LENGTH MATCHES ==================\n",
    "#             X, y= train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "#             # ================= ADD BUILDING FEATURE ================\n",
    "#             X['building'] = i\n",
    "\n",
    "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "\n",
    "#             # ================= SAVING ALL SETS ======================\n",
    "#             self.X_train_sets.append(X_train)\n",
    "#             self.X_test_sets.append(X_test)\n",
    "#             self.y_train_sets.append(y_train)\n",
    "#             self.y_test_sets.append(y_test)\n",
    "#             self.X_pred_sets.append(X_pred)\n",
    "        \n",
    "#     def fit_multi_model(self):\n",
    "        \n",
    "#         self.models = []\n",
    "\n",
    "#         for i in range(len(self.X_train_sets)): \n",
    "            \n",
    "#             train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "\n",
    "#             model = cb.CatBoostClassifier(loss_function=\"Logloss\", logging_level='Silent')\n",
    "\n",
    "#             grid = {'iterations': [100, 150, 200],\n",
    "#                     'learning_rate': [0.03, 0.1],\n",
    "#                     'depth': [2, 4, 6, 8],\n",
    "#                     'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "#             model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "#             self.models.append(model)\n",
    "\n",
    "#     def predict_multi_model(self):\n",
    "#         preds = []\n",
    "        \n",
    "#         for i in range(len(self.X_pred_sets)):\n",
    "#             X_pred = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "#             unformated_pred = self.models[i].predict(X_pred)\n",
    "            \n",
    "#             unformated_pred_df = pd.DataFrame()\n",
    "#             unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "#             unformated_pred_df[\"building\"] = self.X_pred_sets[i][\"building\"]\n",
    "\n",
    "#             replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "#             # Use the replace method with the specified column and dictionary\n",
    "#             unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "#             unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            \n",
    "#             unformated_pred_df.to_csv(self.buildings[i] + \"max_pred\" + \".csv\")\n",
    "\n",
    "#     def get_performance_multi_model(self) -> None:\n",
    "        \n",
    "#         for i in range(len(self.X_test_sets)):\n",
    "#             pred = self.models[i].predict(self.X_test_sets[i])\n",
    "#             accuracy = accuracy_score(self.y_test_sets[i], pred)\n",
    "#             print(\"SCORE BUILDING \" + self.buildings[i])\n",
    "#             print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "#               ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "#               ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "\n",
    "# features = [\"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\", \"ceiling_height_agl:m\", \"clear_sky_energy_1h:J\", \n",
    "#            \"clear_sky_rad:W\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\", \n",
    "#            \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"msl_pressure:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "#            \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", 'day_of_year:day', 'month:month', 'hour:hour']\n",
    "\n",
    "\n",
    "# # Testing the procedure \n",
    "# l = feature_learner(file_paths = file_paths, features = features)\n",
    "# l.create_training_data_multi_model()\n",
    "# l.fit_multi_model()\n",
    "# l.get_performance_multi_model()\n",
    "# l.predict_multi_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "Testing different Machine Learning models, and tuning and adjusting features quickly becomes a bother if one does not create helper function and classes. The code bellow is what the Project Group developed during the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to organize the different steps in the machine learning pipeline. The class contains some nice helper functions\n",
    "that helps the user gain insight into what features the model finds the most usefull.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class learner:\n",
    "    def __init__(self, file_paths: list[list[str]], features: list[str] = [], save_folder: str = \"\", feedback: bool = False) -> None:\n",
    "        self.file_paths = file_paths\n",
    "        self.features = features\n",
    "        self.save_folder = save_folder + \"/\"\n",
    "        self.buildings = [\"A\", \"B\", \"C\"]\n",
    "        self.feedback = feedback\n",
    "\n",
    "    def create_training_data_multi_model(self):\n",
    "        self.X_train_sets = []\n",
    "        self.X_test_sets = []\n",
    "        self.y_train_sets = []\n",
    "        self.y_test_sets = []\n",
    "        self.X_pred_sets = []\n",
    "        self.y_sets = []\n",
    "\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            X = pd.read_csv(path[0], sep='\\t')[self.features]\n",
    "            \n",
    "            y = pd.read_csv(path[1], sep='\\t')['pv_measurement']\n",
    "            \n",
    "            X_pred = pd.read_csv(path[2], sep='\\t')[self.features]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "            print(y)\n",
    "\n",
    "            # ================= SAVING ALL SETS ======================\n",
    "            self.X_train_sets.append(X_train)\n",
    "            self.X_test_sets.append(X_test)\n",
    "            self.y_train_sets.append(y_train)\n",
    "            self.y_test_sets.append(y_test)\n",
    "            self.X_pred_sets.append(X_pred)\n",
    "            self.y_sets.append(y)\n",
    "            \n",
    "\n",
    "\n",
    "    def create_training_data(self):\n",
    "        list_y = []\n",
    "        list_X = []\n",
    "        list_X_pred = []\n",
    "        scalers = []\n",
    "    \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            y = pd.read_parquet(path[0])\n",
    "            X_estimated = pd.read_parquet(path[1])\n",
    "            X_observed = pd.read_parquet(path[2])\n",
    "            X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred = dp.pred_data_processing(X_pred, self.features, self.feedback)\n",
    "            X_pred['building'] = i\n",
    "            \n",
    "            list_X_pred.append(X_pred)\n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            # Pre-process data\n",
    "            y = y.dropna()\n",
    "            X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "            X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "            # BETTER NAME\n",
    "            X, y= dp.train_data_processing(X, y, self.features)\n",
    "            \n",
    "            \n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = i\n",
    "\n",
    "            list_y.append(y)\n",
    "            list_X.append(X)\n",
    "\n",
    "        self.scalers = scalers\n",
    "        # Add all the lists together. However there is a need to add set\n",
    "        y = pd.concat(list_y, axis= 0, ignore_index=True)\n",
    "        X = pd.concat(list_X, axis= 0, ignore_index=True)\n",
    "        X_pred = pd.concat(list_X_pred, axis = 0, ignore_index=True)\n",
    "        \n",
    "        X = X.reset_index(drop=True)\n",
    "        X_pred = X_pred.reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "        \n",
    "        # Should try scaling later ... :/\n",
    "        self.X_train, self.X_test, self.X_pred = X_train, X_test, X_pred\n",
    "    \n",
    "    def fit_multi_model_h2o(self):\n",
    "        \n",
    "        h2o.init()\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(len(self.X_train_sets)):\n",
    "            train_frame =  pd.concat([self.X_train_sets[i], self.y_train_sets[i]], axis = \"columns\")\n",
    "            h2o_frame = h2o.H2OFrame(train_frame)\n",
    "\n",
    "            x_train_columns = h2o_frame.columns\n",
    "\n",
    "            model = H2OAutoML(sort_metric='MAE', max_models=10, exclude_algos=[\"DeepLearning\"])\n",
    "\n",
    "            model.train(x = x_train_columns, y = \"pv_measurement\", training_frame=h2o_frame)\n",
    "            self.models.append(model)\n",
    "\n",
    "            \n",
    "\n",
    "    def fit_multi_model(self):\n",
    "        \n",
    "        self.models = []\n",
    "\n",
    "        for i in range(len(self.X_train_sets)): \n",
    "            \n",
    "            train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "\n",
    "            model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "            grid = {'iterations': [100, 150, 200],\n",
    "                    'learning_rate': [0.03, 0.1],\n",
    "                    'depth': [2, 4, 6, 8],\n",
    "                    'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "            model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "            self.models.append(model)\n",
    "        return self.models\n",
    "\n",
    "    def get_performance_multi_model_h2o(self, models = []):\n",
    "        mae_sum = 0\n",
    "        if len(models) > 0:\n",
    "            for i in range(len(self.X_test_sets)):\n",
    "                pred = models[i].predict(X_test)\n",
    "                mae = (mean_absolute_error(self.y_test_sets[i], pred))\n",
    "                mae_sum = mae + mae_sum\n",
    "\n",
    "        else:    \n",
    "            for i in range(len(self.X_test_sets)):\n",
    "                pred = self.models[i].predict(X_test)\n",
    "                mae = (mean_absolute_error(self.y_test_sets[i], pred))\n",
    "                mae_sum = mae + mae_sum\n",
    "\n",
    "    def predict_multi_model_h2o(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "\n",
    "            X_pred = h2o.H2OFrame(self.X_pred_sets[i])\n",
    "            unformated_pred = self.models[i].predict(X_pred)\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.X_pred_sets[i][\"building\"]\n",
    "\n",
    "            replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "            \n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "            \n",
    "\n",
    "    def predict_multi_model(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            if self.feedback:\n",
    "                X_pred_without_feedback = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "                pred_list = []\n",
    "                # First feedback\n",
    "\n",
    "                \n",
    "                y_df = self.y_sets[i].to_frame()\n",
    "\n",
    "                feedback_init_row = shifted_values_24_h(y_df, \"pv_measurement\").iloc[-1]\n",
    "                \n",
    "                feedback_row = feedback_init_row\n",
    "\n",
    "                \n",
    "\n",
    "                for j, idx in enumerate(X_pred_without_feedback.index.to_list()):\n",
    "                   \n",
    "                    X_pred_with_feedback = pd.concat([X_pred_without_feedback.loc[idx].reset_index(drop=True), feedback_row.reset_index(drop=True)], axis=0, ignore_index=True)\n",
    "                    \n",
    "                    feedback = self.models[i].predict(X_pred_with_feedback)\n",
    "                    \n",
    "                    # Shift the values to the right\n",
    "                    feedback_row.iloc[1:] = feedback_row.iloc[:-1].values\n",
    "                    \n",
    "                    feedback_row.iloc[0] = feedback\n",
    "                    pred_list.append(feedback)\n",
    "\n",
    "                unformated_pred = np.array(pred_list)\n",
    "                \n",
    "                series = pd.Series(unformated_pred)\n",
    "                series.plot()\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                X_pred = self.X_pred_sets[i]\n",
    "                unformated_pred = self.models[i].predict(X_pred)\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.X_pred_sets[i][\"building\"]\n",
    "\n",
    "            replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "            \n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def get_performance_multi_model(self) -> None:\n",
    "        mae_sum = 0\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            pred = self.models[i].predict(self.X_test_sets[i])\n",
    "            pd.Series(pred).plot()\n",
    "            pd.Series(self.y_test_sets[i]).plot()\n",
    "            plt.show()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], pred))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"Mean absolute error: \", mae_sum/len(self.X_test_sets))\n",
    "\n",
    "\n",
    "\n",
    "    def fit_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Based on the selected model the class switches between what model is doing the learning. \n",
    "        \"\"\"\n",
    "\n",
    "        #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "        # Add a function that picks between different models, and processes the data based on this\n",
    "        train_dataset = cb.Pool(self.X_train, self.y_train)\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "        grid = {'iterations': [100, 150, 200],\n",
    "                'learning_rate': [0.03, 0.1],\n",
    "                'depth': [2, 4, 6, 8],\n",
    "                'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "        self.model.grid_search(grid, train_dataset, verbose=False)\n",
    "        \n",
    "\n",
    "    def get_performance(self) -> None:\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        pd.Series(pred).plot()\n",
    "        pd.Series(self.y_test).plot()\n",
    "        \n",
    "        mae = (mean_absolute_error(self.y_test, pred))\n",
    "        print(\"Mean Abs: {:.2f}\".format(mae))\n",
    "\n",
    "    def predict(self) -> None:\n",
    "        \n",
    "        X_pred = self.X_pred.drop('date_forecast', axis = 1)\n",
    "        unformated_pred = self.model.predict(X_pred)\n",
    "        \n",
    "        unformated_pred_df = pd.DataFrame()\n",
    "        unformated_pred_df[\"date_forecast\"] = self.X_pred[\"date_forecast\"]\n",
    "        unformated_pred_df[\"building\"] = self.X_pred[\"building\"]\n",
    "\n",
    "        replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "        # Use the replace method with the specified column and dictionary\n",
    "        unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "        unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "        unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "    \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def save_best_features(self, filename: str, N: int = 0):\n",
    "        if N == 0:\n",
    "            N = len(self.X_test_sets[0].columns.values) - 1\n",
    "        best_features_df = pd.DataFrame()\n",
    "\n",
    "        for i, X in enumerate(self.X_test_sets):\n",
    "           \n",
    "            feature_importance = self.models[i].get_feature_importance()\n",
    "\n",
    "            # Pair feature names with their importance scores\n",
    "            feature_importance_dict = dict(zip(self.models[i].feature_names_, feature_importance))\n",
    "\n",
    "            # Sort features by importance\n",
    "            sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Print or use the top features\n",
    "            top_features = sorted_feature_importance[:N]  # Replace N with the number of top features you want\n",
    "            \n",
    "            # Saving to list\n",
    "            labels = list(X.columns.values)\n",
    "            best_features = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                best_features.append(feat[0])\n",
    "            \n",
    "            best_features_df[\"Model \" + self.buildings[i]] = pd.Series(np.array(best_features))\n",
    "\n",
    "        best_features_df.to_csv(\"tests/\" + self.save_folder + filename + '.csv', sep ='\\t')\n",
    "\n",
    "\n",
    "    def _format_predictions(self, unformated_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # \n",
    "        to_be_submitted_index = pd.read_csv(\"test.csv\")\n",
    "\n",
    "        #convert the \"time\" column to datetime\n",
    "        to_be_submitted_index[\"time\"] = pd.to_datetime(to_be_submitted_index[\"time\"])\n",
    "        pred = pd.merge(unformated_pred, to_be_submitted_index, how='inner', left_on=['date_forecast', 'building'], right_on=[\"time\", \"location\"])\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "        \n",
    "    def _save_predictions(self, pred: pd.DataFrame)->None:\n",
    "        #Make the index and pv_measurement column into a csv file\n",
    "        pred[[\"id\", \"pv_measurement\"]].rename(columns={\"id\" : \"id\" , \"pv_measurement\" : \"prediction\"}).to_csv(\"tests/\" + self.save_folder + \"model_pred.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the user may apply their testing setup. Here they may apply what features they need, and the model will ensure the rest.\n",
    "\n",
    "TODO: Dot signals\n",
    "\n",
    "TODO: Add log of signals\n",
    "\n",
    "TODO: Sin of signals and dot them :/\n",
    "\n",
    "TODO: Test adding the best features.\n",
    "\n",
    "IDEA: The directions may be helpfull. Create a number that tells us more about the sun position. Break it down into breakpoints between them /:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.00\n",
      "1         0.00\n",
      "2         0.00\n",
      "3         0.00\n",
      "4        19.36\n",
      "         ...  \n",
      "34056     9.02\n",
      "34057     0.00\n",
      "34058     0.00\n",
      "34059     0.00\n",
      "34060     0.00\n",
      "Name: pv_measurement, Length: 34061, dtype: float64\n",
      "0        0.000000\n",
      "1        0.000000\n",
      "2        0.000000\n",
      "3        0.000000\n",
      "4        0.000000\n",
      "           ...   \n",
      "32814    0.828587\n",
      "32815   -0.000000\n",
      "32816   -0.000000\n",
      "32817   -0.000000\n",
      "32818   -0.000000\n",
      "Name: pv_measurement, Length: 32819, dtype: float64\n",
      "0        137.20\n",
      "1          0.00\n",
      "2          0.00\n",
      "3          0.00\n",
      "4          0.00\n",
      "          ...  \n",
      "26066     50.96\n",
      "26067      2.94\n",
      "26068      0.00\n",
      "26069     -0.00\n",
      "26070     -0.00\n",
      "Name: pv_measurement, Length: 26071, dtype: float64\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>25 mins 56 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Europe/Berlin</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.44.0.1</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>22 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_asber_l4f23e</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>2.784 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.12 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         25 mins 56 secs\n",
       "H2O_cluster_timezone:       Europe/Berlin\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.44.0.1\n",
       "H2O_cluster_version_age:    22 days\n",
       "H2O_cluster_name:           H2O_from_python_asber_l4f23e\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    2.784 Gb\n",
       "H2O_cluster_total_cores:    8\n",
       "H2O_cluster_allowed_cores:  8\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://localhost:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.12 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || (done) 100%\n",
      "AutoML progress: |\n",
      "14:05:37.910: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "| (done) 100%\n",
      "Parse progress: || (done) 100%\n",
      "AutoML progress: |\n",
      "14:10:35.7: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "| (done) 100%\n",
      "Parse progress: || (done) 100%\n",
      "AutoML progress: |\n",
      "14:14:29.730: AutoML: XGBoost is not available; skipping it.\n",
      "\n",
      "| (done) 100%\n"
     ]
    }
   ],
   "source": [
    "# features = ['direct_rad:W', 'is_in_shadow:idx', 'total_cloud_cover:p', 'sun_elevation:d', \n",
    "#             'derivative_direct_rad:W_d', 'derivative_sun_elevation:d_d', \n",
    "#             'sun_azimuth:d', 't_1000hPa:K', \n",
    "#             'derivative_sun_azimuth:d_d', 'derivative_t_1000hPa:K_d',\n",
    "#             'day_of_year:day', 'month:month', 'hour:hour']\n",
    "\n",
    "# \n",
    "\n",
    "features = [\"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\", \"ceiling_height_agl:m\", \"clear_sky_energy_1h:J\",\n",
    "           \"clear_sky_rad:W\", \"cloud_base_agl:m\", \"dew_or_rime:idx\", \"dew_point_2m:K\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\",\n",
    "           \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"is_in_shadow:idx\", \"msl_pressure:hPa\",\n",
    "           \"precip_5min:mm\", \"pressure_100m:hPa\", \"pressure_50m:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "           \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", \"visibility:m\",\n",
    "           \"wind_speed_10m:ms\"]\n",
    "\n",
    "\n",
    "# Should add features etc, but a bit stranve\n",
    "\n",
    "# Testing the procedure \n",
    "l = learner(file_paths = file_paths, features = features)\n",
    "l.create_training_data_multi_model()\n",
    "models = l.fit_multi_model_h2o()\n",
    "\n",
    "# IDEA: Add a different scaler for all the different features, then transform then into the right scale after the fact\n",
    "# Remove any needless features, add until you get a good outpu\n",
    "# Probable reason for the bad score is the scaling defeciencies.\n",
    "# Get the smallest value that is NoN zero, and cap them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asber\\OneDrive\\Dokumenter\\UNI\\2023 Autumn\\TDT4173 Machine Learning\\group-project\\TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting\\data\\machine_learning.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m l\u001b[39m.\u001b[39;49mget_performance_multi_model_h2o()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m l\u001b[39m.\u001b[39mpredict_multi_model_h2o()\n",
      "\u001b[1;32mc:\\Users\\asber\\OneDrive\\Dokumenter\\UNI\\2023 Autumn\\TDT4173 Machine Learning\\group-project\\TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting\\data\\machine_learning.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m mae_sum \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_test_sets)):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m     mae \u001b[39m=\u001b[39m (mean_absolute_error(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_test_sets[i], pred))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X25sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m     mae_sum \u001b[39m=\u001b[39m mae \u001b[39m+\u001b[39m mae_sum\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "l.get_performance_multi_model_h2o()\n",
    "l.predict_multi_model_h2o()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n"
     ]
    },
    {
     "ename": "H2OStartupError",
     "evalue": "Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mH2OConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\h2o.py:269\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39;49mopen(url\u001b[39m=\u001b[39;49murl, ip\u001b[39m=\u001b[39;49mip, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname, https\u001b[39m=\u001b[39;49mhttps,\n\u001b[0;32m    270\u001b[0m                                  verify_ssl_certificates\u001b[39m=\u001b[39;49mverify_ssl_certificates, cacert\u001b[39m=\u001b[39;49mcacert,\n\u001b[0;32m    271\u001b[0m                                  auth\u001b[39m=\u001b[39;49mauth, proxy\u001b[39m=\u001b[39;49mproxy, cookies\u001b[39m=\u001b[39;49mcookies, verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    272\u001b[0m                                  msgs\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mChecking whether there is an H2O instance running at \u001b[39;49m\u001b[39m{url}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    273\u001b[0m                                        \u001b[39m\"\u001b[39;49m\u001b[39mconnected.\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mnot found.\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    274\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39;49msvc)\n\u001b[0;32m    275\u001b[0m \u001b[39mexcept\u001b[39;00m H2OConnectionError:\n\u001b[0;32m    276\u001b[0m     \u001b[39m# Backward compatibility: in init() port parameter really meant \"baseport\" when starting a local server...\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\backend\\connection.py:406\u001b[0m, in \u001b[0;36mH2OConnection.open\u001b[1;34m(server, url, ip, port, name, https, auth, verify_ssl_certificates, cacert, proxy, cookies, verbose, msgs, strict_version_check)\u001b[0m\n\u001b[0;32m    405\u001b[0m conn\u001b[39m.\u001b[39m_timeout \u001b[39m=\u001b[39m \u001b[39m3.0\u001b[39m\n\u001b[1;32m--> 406\u001b[0m conn\u001b[39m.\u001b[39m_cluster \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49m_test_connection(retries, messages\u001b[39m=\u001b[39;49mmsgs)\n\u001b[0;32m    407\u001b[0m \u001b[39m# If a server is unable to respond within 1s, it should be considered a bug. However we disable this\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# setting for now, for no good reason other than to ignore all those bugs :(\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\backend\\connection.py:713\u001b[0m, in \u001b[0;36mH2OConnection._test_connection\u001b[1;34m(self, max_retries, messages)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 713\u001b[0m     \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m\"\u001b[39m\u001b[39mCould not establish link to the H2O cloud \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m after \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m retries\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m                              \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_url, max_retries, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(errors)))\n",
      "\u001b[1;31mH2OConnectionError\u001b[0m: Could not establish link to the H2O cloud http://localhost:54321 after 5 retries\n[03:47.57] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B686CAD420>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n[03:51.89] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B686CADC00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n[03:56.23] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B686CAE440>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n[04:00.55] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B686CAEC80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n[04:04.85] H2OConnectionError: Unexpected HTTP error: HTTPConnectionPool(host='localhost', port=54321): Max retries exceeded with url: /3/Metadata/schemas/CloudV3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B686CAE2C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mH2OStartupError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\asber\\OneDrive\\Dokumenter\\UNI\\2023 Autumn\\TDT4173 Machine Learning\\group-project\\TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting\\data\\machine_learning.ipynb Cell 19\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X66sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mh2o\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautoml\u001b[39;00m \u001b[39mimport\u001b[39;00m H2OAutoML \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X66sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m OneHotEncoder, LabelEncoder, StandardScaler, OrdinalEncoder\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X66sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m h2o\u001b[39m.\u001b[39;49minit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X66sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m h2o_frame \u001b[39m=\u001b[39m h2o\u001b[39m.\u001b[39mH2OFrame(X_train_sub)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/asber/OneDrive/Dokumenter/UNI/2023%20Autumn/TDT4173%20Machine%20Learning/group-project/TDT4173-Machine-Learning-Solar-Energy-Production-Forecasting/data/machine_learning.ipynb#X66sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m x_train \u001b[39m=\u001b[39m h2o_frame\u001b[39m.\u001b[39mcolumns\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\h2o.py:286\u001b[0m, in \u001b[0;36minit\u001b[1;34m(url, ip, port, name, https, cacert, insecure, username, password, cookies, proxy, start_h2o, nthreads, ice_root, log_dir, log_level, max_log_file_size, enable_assertions, max_mem_size, min_mem_size, strict_version_check, ignore_config, extra_classpath, jvm_custom_args, bind_to_localhost, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m https:\n\u001b[0;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m H2OConnectionError(\u001b[39m'\u001b[39m\u001b[39mStarting local server is not available with https enabled. You may start local\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    284\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m instance of H2O with https manually \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    285\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39m(https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#new-user-quick-start).\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 286\u001b[0m     hs \u001b[39m=\u001b[39m H2OLocalServer\u001b[39m.\u001b[39;49mstart(nthreads\u001b[39m=\u001b[39;49mnthreads, enable_assertions\u001b[39m=\u001b[39;49menable_assertions, max_mem_size\u001b[39m=\u001b[39;49mmmax,\n\u001b[0;32m    287\u001b[0m                               min_mem_size\u001b[39m=\u001b[39;49mmmin, ice_root\u001b[39m=\u001b[39;49mice_root, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level,\n\u001b[0;32m    288\u001b[0m                               max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size, port\u001b[39m=\u001b[39;49mport, name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    289\u001b[0m                               extra_classpath\u001b[39m=\u001b[39;49mextra_classpath, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    290\u001b[0m                               bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost,verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    291\u001b[0m     h2oconn \u001b[39m=\u001b[39m H2OConnection\u001b[39m.\u001b[39mopen(server\u001b[39m=\u001b[39mhs, https\u001b[39m=\u001b[39mhttps, verify_ssl_certificates\u001b[39m=\u001b[39mverify_ssl_certificates,\n\u001b[0;32m    292\u001b[0m                                  cacert\u001b[39m=\u001b[39mcacert, auth\u001b[39m=\u001b[39mauth, proxy\u001b[39m=\u001b[39mproxy, cookies\u001b[39m=\u001b[39mcookies, verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    293\u001b[0m                                  strict_version_check\u001b[39m=\u001b[39msvc)\n\u001b[0;32m    294\u001b[0m h2oconn\u001b[39m.\u001b[39mcluster\u001b[39m.\u001b[39mtimezone \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUTC\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\backend\\server.py:139\u001b[0m, in \u001b[0;36mH2OLocalServer.start\u001b[1;34m(jar_path, nthreads, enable_assertions, max_mem_size, min_mem_size, ice_root, log_dir, log_level, max_log_file_size, port, name, extra_classpath, verbose, jvm_custom_args, bind_to_localhost)\u001b[0m\n\u001b[0;32m    136\u001b[0m     hs\u001b[39m.\u001b[39m_tempdir \u001b[39m=\u001b[39m hs\u001b[39m.\u001b[39m_ice_root\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttempting to start a local H2O server...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 139\u001b[0m hs\u001b[39m.\u001b[39;49m_launch_server(port\u001b[39m=\u001b[39;49mport, baseport\u001b[39m=\u001b[39;49mbaseport, nthreads\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(nthreads), ea\u001b[39m=\u001b[39;49menable_assertions,\n\u001b[0;32m    140\u001b[0m                   mmax\u001b[39m=\u001b[39;49mmax_mem_size, mmin\u001b[39m=\u001b[39;49mmin_mem_size, jvm_custom_args\u001b[39m=\u001b[39;49mjvm_custom_args,\n\u001b[0;32m    141\u001b[0m                   bind_to_localhost\u001b[39m=\u001b[39;49mbind_to_localhost, log_dir\u001b[39m=\u001b[39;49mlog_dir, log_level\u001b[39m=\u001b[39;49mlog_level, max_log_file_size\u001b[39m=\u001b[39;49mmax_log_file_size)\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m verbose: \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m  Server is running at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (hs\u001b[39m.\u001b[39mscheme, hs\u001b[39m.\u001b[39mip, hs\u001b[39m.\u001b[39mport))\n\u001b[0;32m    143\u001b[0m atexit\u001b[39m.\u001b[39mregister(\u001b[39mlambda\u001b[39;00m: hs\u001b[39m.\u001b[39mshutdown())\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\backend\\server.py:271\u001b[0m, in \u001b[0;36mH2OLocalServer._launch_server\u001b[1;34m(self, port, baseport, mmax, mmin, ea, nthreads, jvm_custom_args, bind_to_localhost, log_dir, log_level, max_log_file_size)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ip \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m127.0.0.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    270\u001b[0m \u001b[39m# Find Java and check version. (Note that subprocess.check_output returns the output as a bytes object)\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m java \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_java()\n\u001b[0;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_java(java, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose)\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
      "File \u001b[1;32mc:\\Users\\asber\\anaconda3\\envs\\basic_machine_learning\\lib\\site-packages\\h2o\\backend\\server.py:443\u001b[0m, in \u001b[0;36mH2OLocalServer._find_java\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[39mreturn\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dirpath, java)\n\u001b[0;32m    442\u001b[0m \u001b[39m# not found...\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m \u001b[39mraise\u001b[39;00m H2OStartupError(\u001b[39m\"\u001b[39m\u001b[39mCannot find Java. Please install the latest JRE from\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mH2OStartupError\u001b[0m: Cannot find Java. Please install the latest JRE from\nhttp://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html#java-requirements"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_pred = h2o.as_list(y_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
