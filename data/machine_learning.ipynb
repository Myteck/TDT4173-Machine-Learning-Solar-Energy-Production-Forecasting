{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import data_pipeline as dp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model\n",
    "import catboost as cb\n",
    "\n",
    "# Data Processing Tools\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from enum import Enum\n",
    " \n",
    "# TODO: ADD NICE MODEL SELECTIONS\n",
    "# TODO: ADD FILTERING TOOLS\n",
    "# TODO: ADD FREQUENCY GENERATION\n",
    "\n",
    "class learner:\n",
    "    def __init__(self, file_paths: list[list[str]], features: list[str] = [], learning_algorithm: str = cb.CatBoostRegressor) -> None:\n",
    "        self.file_paths = file_paths\n",
    "        self.features = features\n",
    "        self.learning_algorithm = learning_algorithm\n",
    "        self.buildings = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "    def create_training_data(self):\n",
    "        list_y = []\n",
    "        list_X = []\n",
    "        list_X_pred = []\n",
    "        scalers = []\n",
    "    \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            print(i)\n",
    "            y = pd.read_parquet(path[0])\n",
    "            X_estimated = pd.read_parquet(path[1])\n",
    "            X_observed = pd.read_parquet(path[2])\n",
    "            X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred = dp.pred_data_processing(X_pred, self.features)\n",
    "            X_pred['building'] = i\n",
    "            print(\"These are XPREDs features\", X_pred.columns.values)\n",
    "            list_X_pred.append(X_pred)\n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            # Pre-process data\n",
    "            y = y.dropna()\n",
    "            X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "            X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "            \n",
    "            # BETTER NAME\n",
    "            X, y= dp.train_data_processing(X, y, self.features)\n",
    "            \n",
    "            print(y.head())\n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = i\n",
    "\n",
    "            # Create a MinMaxScaler object\n",
    "            scaler = MinMaxScaler()\n",
    "            \n",
    "            # Still a bit strange that we may not scale them :/\n",
    "            # Scaling the signal\n",
    "            # Min-max scaling\n",
    "            # min_val = y.min()\n",
    "            max_val = y.max()\n",
    "            # y = (y - min_val) / (max_val - min_val)\n",
    "            \n",
    "            # scaler = (max_val - min_val) + min_val\n",
    "\n",
    "            scalers.append(max_val)\n",
    "\n",
    "            # Adding the datasets to the lists\n",
    "            list_y.append(y)\n",
    "            list_X.append(X)\n",
    "\n",
    "        self.scalers = scalers\n",
    "        # Add all the lists together. However there is a need to add set\n",
    "        y = pd.concat(list_y, axis= 0)\n",
    "        y.info()\n",
    "        X = pd.concat(list_X, axis= 0)\n",
    "        X_pred = pd.concat(list_X_pred, axis = 0)\n",
    "\n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "        \n",
    "        # Should probably save their date indexes just in case :/\n",
    "\n",
    "        # Should try scaling later ... :/\n",
    "        self.X_train, self.X_test, self.X_pred = X_train, X_test, X_pred\n",
    "        print(\"These are self XPREDs features\", self.X_pred.columns.values)\n",
    "        return None\n",
    "\n",
    "    def _scale_sets(self, X_train: pd.DataFrame, X_test: pd.DataFrame, X_pred: pd.DataFrame):\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        unscaled_X_train = X_train.drop('building', axis = 1)\n",
    "        X_train_scaled_values = scaler.fit_transform(unscaled_X_train.values)\n",
    "\n",
    "        X_train_scaled['building'] = X_train['building']\n",
    "\n",
    "        unscaled_X_test = X_test.drop('building', axis = 1)\n",
    "        X_test_scaled = scaler.transform(unscaled_X_test)\n",
    "        X_test_scaled['building'] = X_test['building']\n",
    "        \n",
    "        unscaled_X_pred = X_pred.drop('building', axis = 1)\n",
    "        X_pred_scaled = scaler.transform(unscaled_X_pred)\n",
    "        X_pred_scaled['building'] = X_pred['building']\n",
    "        \n",
    "        return X_train, X_test, X_pred\n",
    "\n",
    "\n",
    "    def fit_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Based on the selected model the class switches between what model is doing the learning. \n",
    "        \"\"\"\n",
    "\n",
    "        #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "        # Add a function that picks between different models, and processes the data based on this\n",
    "        train_dataset = cb.Pool(self.X_train, self.y_train)\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "        grid = {'iterations': [100, 150, 200],\n",
    "                'learning_rate': [0.03, 0.1],\n",
    "                'depth': [2, 4, 6, 8],\n",
    "                'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "        self.model.grid_search(grid, train_dataset, verbose=False)\n",
    "        \n",
    "\n",
    "    def get_performance(self) -> None:\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        mae = (mean_absolute_error(self.y_test, pred))\n",
    "        print(\"Mean Abs: {:.2f}\".format(mae))\n",
    "\n",
    "    def predict(self) -> None:\n",
    "        print(self.X_pred.columns.values)\n",
    "        X_pred = self.X_pred.drop('date_forecast', axis = 1)\n",
    "        unformated_pred = self.model.predict(X_pred)\n",
    "        \n",
    "        unformated_pred_df = pd.DataFrame()\n",
    "        unformated_pred_df[\"date_forecast\"] = self.X_pred[\"date_forecast\"]\n",
    "        unformated_pred_df[\"building\"] = self.X_pred[\"building\"]\n",
    "\n",
    "        replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "         # Use the replace method with the specified column and dictionary\n",
    "        unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "        unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "        unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "        \n",
    "\n",
    "        # Now we want to rescale each signal appropriatly :/\n",
    "        unformated_pred_df = self._rescale(unformated_pred_df)\n",
    "        \n",
    "        unformated_pred_df.to_csv(\"sacfadasx.csv\", sep='\\t')\n",
    "\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        unformated_pred_df[\"scaled_pv_measurements\"].plot()\n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "        \n",
    "    def _rescale(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for i, building in enumerate(self.buildings):\n",
    "            \n",
    "            values_to_scale = df['pv_measurement'].loc[df['building'] == building]\n",
    "            max_new = values_to_scale.max()\n",
    "            \n",
    "            df.loc[df['building'] == building, \"scaled_pv_measurements\"] =  self.scalers[i]*values_to_scale/max_new\n",
    "            # # Resclaing does not work... :/\n",
    "            # for idx, val in zip(building_indexes_df.index.to_list(), list(values_to_scale)):\n",
    "            #     df[]'pv_measurement'] = self.scalers[i]*val/max_new\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _format_predictions(self, unformated_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # \n",
    "        to_be_submitted_index = pd.read_csv(\"test.csv\")\n",
    "\n",
    "        #convert the \"time\" column to datetime\n",
    "        to_be_submitted_index[\"time\"] = pd.to_datetime(to_be_submitted_index[\"time\"])\n",
    "        pred = pd.merge(unformated_pred, to_be_submitted_index, how='inner', left_on=['date_forecast', 'building'], right_on=[\"time\", \"location\"])\n",
    "        print(len(unformated_pred.index))\n",
    "        return pred\n",
    "        \n",
    "        return None\n",
    "    def _save_predictions(self, pred: pd.DataFrame)->None:\n",
    "        #Make the index and pv_measurement column into a csv file\n",
    "        pred[[\"id\", \"pv_measurement\"]].rename(columns={\"id\" : \"id\" , \"pv_measurement\" : \"prediction\"}).to_csv(\"model_pred.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "              ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "              ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "These are XPREDs features ['direct_rad:W' 'date_forecast' 'building']\n",
      "0     0.00\n",
      "1     0.00\n",
      "2     0.00\n",
      "3     0.00\n",
      "4    19.36\n",
      "Name: pv_measurement, dtype: float64\n",
      "1\n",
      "These are XPREDs features ['direct_rad:W' 'date_forecast' 'building']\n",
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: pv_measurement, dtype: float64\n",
      "2\n",
      "These are XPREDs features ['direct_rad:W' 'date_forecast' 'building']\n",
      "0    137.2\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "Name: pv_measurement, dtype: float64\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 92951 entries, 0 to 26070\n",
      "Series name: pv_measurement\n",
      "Non-Null Count  Dtype  \n",
      "--------------  -----  \n",
      "92951 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.4 MB\n",
      "These are self XPREDs features ['direct_rad:W' 'date_forecast' 'building']\n"
     ]
    }
   ],
   "source": [
    "features = ['direct_rad:W'] #, 'is_in_shadow:idx', 'total_cloud_cover:p', 'sun_elevation:d', 'sun_azimuth:d', 't_1000hPa:K']\n",
    " \n",
    "\n",
    "#[\"absolute_humidity_2m:gm3\", \"air_density_2m:kgm3\", \"ceiling_height_agl:m\", \"clear_sky_energy_1h:J\", \n",
    "#            \"clear_sky_rad:W\", \"cloud_base_agl:m\", \"dew_or_rime:idx\", \"dew_point_2m:K\", \"diffuse_rad:W\", \"diffuse_rad_1h:J\", \n",
    "#            \"direct_rad:W\", \"direct_rad_1h:J\", \"effective_cloud_cover:p\", \"is_in_shadow:idx\", \"msl_pressure:hPa\", \n",
    "#            \"precip_5min:mm\", \"pressure_100m:hPa\", \"pressure_50m:hPa\", \"prob_rime:p\", \"rain_water:kgm2\", \"relative_humidity_1000hPa:p\",\n",
    "#            \"sun_azimuth:d\", \"sun_elevation:d\", \"super_cooled_liquid_water:kgm2\", \"t_1000hPa:K\", \"total_cloud_cover:p\", \"visibility:m\",\n",
    "#            \"wind_speed_10m:ms\"] \n",
    "# Should add features etc, but a bit stranve\n",
    "\n",
    "# Testing the procedure \n",
    "l = learner(file_paths = file_paths, features = features)\n",
    "l.create_training_data()\n",
    "l.fit_model()\n",
    "l.get_performance()\n",
    "l.predict()\n",
    "\n",
    "# IDEA: Add a different scaler for all the different features, then transform then into the right scale after the fact\n",
    "# Remove any needless features, add until you get a good outpu\n",
    "# Probable reason for the bad score is the scaling defeciencies.\n",
    "# Get the smallest value that is NoN zero, and cap them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
