{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O\n",
    "This Jupyter notebook contains all necessary steps to reproduce the projects group Kaggle submission. Notice that you would need the \"parquet files\" in the same folder structure as the original data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning Models\n",
    "import catboost as cb\n",
    "\n",
    "# Machine Learning Tools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_length_matching(train: pd.DataFrame, obs: pd.DataFrame)-> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function is intended to ensure that both the training data and\n",
    "    the observed data are sorted, and contain the same number of entries. \n",
    "    \"\"\"\n",
    "\n",
    "    # Cut the data frames so that their date match.\n",
    "    obs_feature_test = obs[obs['date_forecast'].isin(train['time'])].sort_values(by=['date_forecast'])  # sortert etter datao\n",
    "    # If only one of them has the date ensure that the other also has the same sorting.\n",
    "    train_feature_test = train[train['time'].isin(obs['date_forecast'])].sort_values(by=['time'])       # sortert etter datao\n",
    "\n",
    "    return train_feature_test, obs_feature_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]):\n",
    "    squared_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        squared_df['squared_' + measurement + '_2'] = df[measurement]**2\n",
    "    return squared_df\n",
    "\n",
    "def upscale_(df: pd.DataFrame, feature: str, upscale: int) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    upscale_df = pd.DataFrame()\n",
    "    \n",
    "    upscale_df[\"uscale_\" + feature] = df[feature]*upscale\n",
    "\n",
    "    return upscale_df\n",
    "\n",
    "def dot_df(df: pd.DataFrame, dot_feature: str, features: list[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    dot_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        dot_df[dot_feature + '_dot_' + feature] = df[dot_feature] * df[feature]\n",
    "\n",
    "    return dot_df\n",
    "\n",
    "def log_df(df: pd.DataFrame, features: list[str]):\n",
    "    \n",
    "    df = df.copy()\n",
    "    log_df = pd.DataFrame()\n",
    "\n",
    "    for feature in features:\n",
    "        df[feature] = abs(df[feature])\n",
    "        df[feature] = df[feature] + 1\n",
    "        log_df['log_' + feature] =  np.log(df[feature])\n",
    "\n",
    "    return log_df\n",
    "\n",
    "\n",
    "def difference_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    der_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate derivative estimates\n",
    "        der_df['derivative_' + measurement + '_d'] = df[measurement].diff()\n",
    "    \n",
    "    df = df.drop('time_diff', axis =  1)\n",
    "\n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    der_df = der_df.interpolate(method='linear')\n",
    "    der_df = der_df.bfill()\n",
    "    \n",
    "    return der_df\n",
    "\n",
    "def double_derivative_from_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a derivative column to the pandas dataframe. May be used to create time dependency.\n",
    "    \"\"\"\n",
    "    dder_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps) \n",
    "\n",
    "    # Calculate time differences\n",
    "    df['time_diff'] = df[timeStamps].diff()\n",
    "\n",
    "    # Calculate derivative estimates\n",
    "    for measurement in measurements:\n",
    "        dder_df['double_derivative_' + measurement + '_dd'] = df[measurement].diff() / (divmod(df['time_diff'].dt.total_seconds(), 60)[0]**2)\n",
    "    \n",
    "    df = df.drop('time_diff', axis=1)\n",
    "    \n",
    "    # Since the first element will result in a NaN, we must backfill this one.\n",
    "    dder_df = dder_df.interpolate(method='linear')\n",
    "    dder_df = dder_df.bfill()\n",
    "\n",
    "    return dder_df\n",
    "\n",
    "def daily_accumulated_val_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    i_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        i_df['integral_' + measurement + '_integral'] = df.groupby('date')[measurement].cumsum()\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return i_df\n",
    "\n",
    "def daily_accumulated_val_squared_df(df: pd.DataFrame, timeStamps: str, measurements: list[str]) -> pd.DataFrame:\n",
    "    \n",
    "    di_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    # Sort DataFrame by timestamp\n",
    "    df = df.sort_values(timeStamps)\n",
    "\n",
    "    # Create a new column for the date\n",
    "    df['date'] = df[timeStamps].dt.date\n",
    "\n",
    "    for measurement in measurements:\n",
    "        # Calculate the integral value for each day\n",
    "        di_df['double_integral_' + measurement + '_dintegral'] = df.groupby('date')[measurement].cumsum()**2\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    return di_df\n",
    "\n",
    "def time_data_from_df(df: pd.DataFrame, timestamps: str) -> pd.DataFrame: \n",
    "    # Extracting components\n",
    "    time_df = pd.DataFrame()\n",
    "    df = df.copy()\n",
    "    time_df['day_of_year:day'] = df[timestamps].dt.dayofyear\n",
    "    time_df['month:month'] = df[timestamps].dt.month\n",
    "    #time_df['year:year'] = df[timestamps].dt.year\n",
    "    time_df['hour:hour'] = df[timestamps].dt.hour\n",
    "    return time_df\n",
    "\n",
    "\n",
    "# Should modify this\n",
    "def n_largest_freq(df: pd.DataFrame, measurements: list[str], n_largest: int):\n",
    "    \"\"\"\n",
    "    Generates values based on the largest frequencies that are present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    freq_df = pd.DataFrame()\n",
    "\n",
    "    for measurement in measurements:\n",
    "        signal = df[measurement].values\n",
    "\n",
    "        fft_result = np.fft.fft(signal)\n",
    "        \n",
    "        \n",
    "        indices = np.argsort(np.abs(fft_result))[::-1][:n_largest]\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            # Set all other frequency components to zero\n",
    "            fft_result_filtered = np.zeros_like(fft_result)\n",
    "            fft_result_filtered[idx] = fft_result[idx]\n",
    "\n",
    "            # Compute IFFT\n",
    "            ifft_result = np.fft.ifft(fft_result_filtered)\n",
    "\n",
    "            # Add the filtered results to the dataframe\n",
    "            freq_df[\"filtered_freq_\" + str(i) +\"_\" + measurement] = ifft_result.real\n",
    "\n",
    "\n",
    "    return freq_df\n",
    "\n",
    "def freq_comb(df: pd.DataFrame, features: list[str]) -> np.array:\n",
    "    \"\"\"\n",
    "    Takes the fourier transform of multiple signals add them together, and then takes the inverse.\n",
    "\n",
    "    features: Are what you would like to combine.\n",
    "    df: Chosen dataframe containing feature information.\n",
    "    \"\"\"\n",
    "\n",
    "    total_fft = 0\n",
    "    \n",
    "    for feat in features:\n",
    "        # Finding the signal directly might be wrong due to timestamps and such, but might still be helpful. It is not correct, but improvements like day by day sampling might be useful.\n",
    "        signal = df[feat].values\n",
    "\n",
    "        # Min-max scaling\n",
    "        scaled_signal = min_max_scale(signal)\n",
    "        \n",
    "        fft = np.fft.fft(scaled_signal)\n",
    "        total_fft = total_fft + fft\n",
    "    \n",
    "    ifft_result = np.fft.ifft(total_fft)\n",
    "\n",
    "    return ifft_result.real\n",
    "\n",
    "def min_max_scale(signal: np.array) -> np.array:\n",
    "    # Calculate min and max values\n",
    "    min_val = np.min(signal)\n",
    "    max_val = np.max(signal)\n",
    "\n",
    "    # Min-max scaling\n",
    "    scaled_signal = (signal - min_val) / (max_val - min_val)\n",
    "\n",
    "    return scaled_signal\n",
    "\n",
    "def shifted_values_24_h(y: pd.DataFrame, measurement: str)->pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1, 25):\n",
    "        df[measurement + 'n-' + str(i)] = y[measurement].shift(i)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_features(df: pd.DataFrame):\n",
    "    # Extract the part before \":\" in column names\n",
    "    df.columns = df.columns.str.split(':').str[0]\n",
    "\n",
    "    # Group by modified column names and sum values\n",
    "    grouped_df = df.groupby(df.columns, axis=1).sum()\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "def duplicates(df: pd.DataFrame)->None:\n",
    "    df = df.copy()\n",
    "    # Assuming df is your DataFrame and 'column_name' is the column you're interested in\n",
    "    duplicate_counts = df['pv_measurement'].value_counts()\n",
    "    duplicate_counts = duplicate_counts[duplicate_counts > 1]\n",
    "\n",
    "    print(duplicate_counts)\n",
    "\n",
    "def remove_double_entries(y: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Assuming df is your DataFrame and 'column_name' is the column you're interested in\n",
    "    df = y.copy()\n",
    "    mask = (df['pv_measurement'] != df['pv_measurement'].shift()) | (df['pv_measurement'] == 0)\n",
    "    filtered_df = df[mask]\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A couple of functions to generate the approprate features for both training and prediction data.\n",
    "\"\"\"\n",
    "\n",
    "def train_data_processing(X: pd.DataFrame, y: pd.DataFrame, filter_list: list[str] = [], months: list[int] = [], feedback: bool = False):\n",
    "   \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line. MIGHT BE BAD FOR THE TRAINING DATA DROP USELESSNESS!\n",
    "    X = X.interpolate(method='linear', limit_direction = \"both\")\n",
    "    \n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "    measurements = list(X.columns.values)\n",
    "    \n",
    "    measurements.remove(timestamps)\n",
    "    # print(measurements)\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X, timestamps, measurements)\n",
    "    # print(\"Squared\")\n",
    "    # print(squared_df.columns.values)\n",
    "    \n",
    "    der_df = difference_df(X, timestamps, measurements)\n",
    "    # print(\"Derivative\")\n",
    "    # print(der_df.columns.values)\n",
    "    \n",
    "    dder_df = double_derivative_from_df(X, timestamps, measurements)\n",
    "    # print(\"Double Derivative\")\n",
    "    # print(dder_df.columns.values)\n",
    "    \n",
    "    int_df = daily_accumulated_val_df(X, timestamps, measurements)\n",
    "    # print(\"Integral\")\n",
    "    # print(int_df.columns.values)\n",
    "    \n",
    "    dint_df = daily_accumulated_val_squared_df(X, timestamps, measurements)\n",
    "    # print(\"Double Integral\")\n",
    "    # print(dint_df.columns.values)\n",
    "    \n",
    "    l_df = log_df(X, measurements)\n",
    "    # print(\"Log\")\n",
    "    # print(l_df.columns.values)\n",
    "    \n",
    "    dotted_df = dot_df(X, 'direct_rad:W', measurements)\n",
    "    # print(\"Dotted\")\n",
    "    # print(dotted_df.columns.values)\n",
    "\n",
    "    n_largest_freq_df =  n_largest_freq(X, measurements, n_largest = 5)\n",
    "    # print(\"N-largest\")\n",
    "    # print(n_largest_freq_df.columns.values)\n",
    "    \n",
    "    time_df = time_data_from_df(X, timestamps)\n",
    "    # print(\"Time\")\n",
    "    # print(time_df.columns.values)\n",
    "\n",
    "    X = pd.concat([X, squared_df, der_df, dder_df, dint_df, int_df, l_df, dotted_df, n_largest_freq_df, time_df], axis = \"columns\")\n",
    "    \n",
    "    if len(months) > 0:\n",
    "        X =  X[X['date_forecast'].dt.month.isin(months)]\n",
    "\n",
    "    if len(filter_list) > 0:\n",
    "        X = X[filter_list + [\"date_forecast\"]]\n",
    "\n",
    "    # Additional features\n",
    "    duplicates(y)\n",
    "    y = remove_double_entries(y)\n",
    "    #der_y = difference_df(y, \"time\", [\"pv_measurement\"])\n",
    "    # der_y_shifted = shifted_values_24_h(der_y, \"derivative_pv_measurement_d\")\n",
    "    y_shifted =  shifted_values_24_h(y, \"pv_measurement\")\n",
    "    y.reset_index(drop = True)\n",
    "    y_shifted.reset_index(drop = True)\n",
    "    # Adding together the added features to one dataframe.\n",
    "    y_BIG = pd.concat([y, y_shifted])\n",
    "    X.reset_index(drop = True)\n",
    "\n",
    "    # Making sure that the two dataframes match in length.\n",
    "    y_BIG, X = data_length_matching(y_BIG, X)\n",
    "\n",
    "    # Get our desired output\n",
    "    y = y_BIG[\"pv_measurement\"]\n",
    "   \n",
    "    \n",
    "    if feedback:\n",
    "        # Removing datetime object column.\n",
    "        y_features = y_BIG.drop('pv_measurement', axis=1)\n",
    "        y_features = y_features.drop('time', axis=1)\n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "        y_features = y_features.reset_index(drop = True)\n",
    "        X = X.reset_index(drop = True)\n",
    "        X = pd.concat([X, y_features], axis = 1)\n",
    "        \n",
    "    \n",
    "    # Removing datetime object column\n",
    "    X = X.drop(timestamps, axis=1)\n",
    "    \n",
    "    X = X.reset_index(drop = True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def pred_data_processing(X_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that reads\n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing NaN values. If there are missing values treat start and end points as beginning and end of a line.\n",
    "    X_pred = X_pred.interpolate(method = 'linear')\n",
    "    X_pred = X_pred.bfill()\n",
    "\n",
    "    # Extract necesarry values for feature generation.\n",
    "    timestamps = \"date_forecast\"\n",
    "\n",
    "    # Removing date-time from measurements\n",
    "    measurements = list(X_pred.columns.values)\n",
    "    measurements.remove(\"date_forecast\")\n",
    "    measurements.remove(\"date_calc\")\n",
    "\n",
    "    # Probable features that may be used\n",
    "    squared_df = square_df(X_pred, timestamps, measurements)\n",
    "    der_df = difference_df(X_pred, timestamps, measurements)\n",
    "    dder_df = double_derivative_from_df(X_pred, timestamps, measurements)\n",
    "    int_df = daily_accumulated_val_df(X_pred, timestamps, measurements)\n",
    "    dint_df = daily_accumulated_val_squared_df(X_pred, timestamps, measurements)\n",
    "    l_df = log_df(X_pred, measurements)\n",
    "    dotted_df = dot_df(X_pred, 'direct_rad:W', measurements)\n",
    "    time_df = time_data_from_df(X_pred, timestamps)\n",
    "    n_largest_freq_df =  n_largest_freq(X_pred, measurements, n_largest = 5)\n",
    "\n",
    "    X_pred_new = pd.concat([X_pred, squared_df, der_df, dder_df, dint_df, int_df, l_df, dotted_df, n_largest_freq_df, time_df], axis = \"columns\")\n",
    "\n",
    "    X_pred_new = X_pred_new.drop(\"date_calc\", axis = 1)\n",
    "    return X_pred_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_and_pred_data(file_paths: list[str], months: list[int] = []):\n",
    "    buildings = ['A', 'B', 'C']\n",
    "    paths = []\n",
    "    \n",
    "    for i, path in enumerate(file_paths):\n",
    "        # Retrieve data        \n",
    "        y = pd.read_parquet(path[0])\n",
    "        X_estimated = pd.read_parquet(path[1])\n",
    "        X_observed = pd.read_parquet(path[2])\n",
    "        X_pred = pd.read_parquet(path[3])\n",
    "\n",
    "        # Processing and cleaning data\n",
    "        y = y.dropna()\n",
    "        X_estimated = X_estimated.drop(\"date_calc\", axis = 1)\n",
    "        X = pd.concat([X_observed, X_estimated], axis = 0, ignore_index=True)\n",
    "        \n",
    "        X, y= train_data_processing(X, y, months=months)\n",
    "\n",
    "        X_pred = pred_data_processing(X_pred)\n",
    "        \n",
    "        X_path = buildings[i] + \"/\" + \"X.csv\"\n",
    "        X.to_csv(path_or_buf = X_path, sep='\\t')\n",
    "\n",
    "        y_path = buildings[i] + \"/\" + \"y.csv\"\n",
    "        y.to_csv(path_or_buf = y_path, sep='\\t')\n",
    "\n",
    "        X_pred_path = buildings[i] + \"/\" + \"X_pred.csv\"\n",
    "        X_pred.to_csv(path_or_buf = X_pred_path, sep='\\t')\n",
    "        \n",
    "        \n",
    "        paths.append([X_path, y_path, X_pred_path])\n",
    "\n",
    "    return paths \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [['A/train_targets.parquet', 'A/X_train_estimated.parquet', 'A/X_train_observed.parquet', 'A/X_test_estimated.parquet'],\n",
    "              ['B/train_targets.parquet', 'B/X_train_estimated.parquet', 'B/X_train_observed.parquet', 'B/X_test_estimated.parquet'],\n",
    "              ['C/train_targets.parquet', 'C/X_train_estimated.parquet', 'C/X_train_observed.parquet', 'C/X_test_estimated.parquet']]\n",
    "\n",
    "file_paths = create_training_and_pred_data(file_paths = file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A class to organize the different steps in the machine learning pipeline. The class contains some nice helper functions\n",
    "that helps the user gain insight into what features the model finds the most usefull.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class learner:\n",
    "    def __init__(self, file_paths: list[list[str]], features: list[str] = [], save_folder: str = \"\", feedback: bool = False, categorical_features: list[str] = []) -> None:\n",
    "        self.file_paths = file_paths\n",
    "        self.features = features\n",
    "        self.save_folder = save_folder + \"/\"\n",
    "        self.buildings = [\"A\", \"B\", \"C\"]\n",
    "        self.feedback = feedback\n",
    "        self.categorical_features = categorical_features\n",
    "\n",
    "    def create_training_data_multi_model(self):\n",
    "        self.X_train_sets = []\n",
    "        self.X_test_sets = []\n",
    "        self.y_train_sets = []\n",
    "        self.y_test_sets = []\n",
    "        self.X_pred_sets = []\n",
    "        self.y_sets = []\n",
    "\n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            X = pd.read_csv(path[0], sep='\\t')[self.features]\n",
    "            \n",
    "            y = pd.read_csv(path[1], sep='\\t')['pv_measurement']\n",
    "\n",
    "            X_pred = pd.read_csv(path[2], sep='\\t')[self.features + [\"date_forecast\"]]\n",
    "            X_pred[\"date_forecast\"] = pd.to_datetime(X_pred[\"date_forecast\"])\n",
    "            \n",
    "            for cat_feat in self.categorical_features:\n",
    "                X_pred[cat_feat] = X_pred[cat_feat].astype(int)\n",
    "                X[cat_feat] = X[cat_feat].astype(int)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "            \n",
    "\n",
    "            # ================= SAVING ALL SETS ======================\n",
    "            self.X_train_sets.append(X_train)\n",
    "            self.X_test_sets.append(X_test)\n",
    "            self.y_train_sets.append(y_train)\n",
    "            self.y_test_sets.append(y_test)\n",
    "            self.X_pred_sets.append(X_pred)\n",
    "            self.y_sets.append(y)\n",
    "            \n",
    "\n",
    "\n",
    "    def create_training_data(self):\n",
    "        list_y = []\n",
    "        list_X = []\n",
    "        list_X_pred = []\n",
    "        scalers = []\n",
    "    \n",
    "        for i, path in enumerate(self.file_paths):\n",
    "            \n",
    "            X = pd.read_csv(path[0], sep='\\t')[self.features]\n",
    "            y = pd.read_csv(path[1], sep='\\t')['pv_measurement']\n",
    "            X_pred = pd.read_csv(path[2], sep='\\t')[self.features + [\"date_forecast\"]]\n",
    "            X_pred[\"date_forecast\"] = pd.to_datetime(X_pred[\"date_forecast\"])\n",
    "\n",
    "            y.plot()\n",
    "            plt.show()\n",
    "            # =================  TEST DATA  ================\n",
    "            X_pred['building'] = self.buildings[i]\n",
    "            \n",
    "            list_X_pred.append(X_pred)\n",
    "\n",
    "            # =================TRAINING DATA================\n",
    "            \n",
    "            # ADD A FUNCTION TO GENERATE BUILDING FEATURE.\n",
    "            X['building'] = self.buildings[i]\n",
    "\n",
    "            list_y.append(y)\n",
    "            list_X.append(X)\n",
    "\n",
    "        self.scalers = scalers\n",
    "        # Add all the lists together. However there is a need to add set\n",
    "        y = pd.concat(list_y, axis= 0, ignore_index=True)\n",
    "        X = pd.concat(list_X, axis= 0, ignore_index=True)\n",
    "        X_pred = pd.concat(list_X_pred, axis = 0, ignore_index=True)\n",
    "        \n",
    "        X = X.reset_index(drop=True)\n",
    "        X_pred = X_pred.reset_index(drop=True)\n",
    "\n",
    "        for cat_feat in self.categorical_features:\n",
    "            X_pred[cat_feat] = X_pred[cat_feat].astype(int)\n",
    "            X[cat_feat] = X[cat_feat].astype(int)\n",
    "        \n",
    "        X_train, X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.15, shuffle=True)\n",
    "        \n",
    "        \n",
    "        self.X_train, self.X_test, self.X_pred = X_train, X_test, X_pred\n",
    "    \n",
    "    def fit_multi_model_h2o(self):\n",
    "        \n",
    "        h2o.init()\n",
    "        self.models = []\n",
    "        \n",
    "        for i in range(len(self.X_train_sets)):\n",
    "            train_frame =  pd.concat([self.X_train_sets[i], self.y_train_sets[i]], axis = \"columns\")\n",
    "            h2o_frame = h2o.H2OFrame(train_frame)\n",
    "\n",
    "            x_train_columns = h2o_frame.columns\n",
    "\n",
    "            model = H2OAutoML(sort_metric='MAE', max_models=10, exclude_algos=[\"DeepLearning\"])\n",
    "\n",
    "            model.train(x = x_train_columns, y = \"pv_measurement\", training_frame=h2o_frame)\n",
    "            self.models.append(model)\n",
    "\n",
    "\n",
    "    def fit_multi_model(self):\n",
    "        \n",
    "        self.models = []\n",
    "\n",
    "        for i in range(len(self.X_train_sets)): \n",
    "            \n",
    "            if len(self.categorical_features) > 0: \n",
    "                train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i], cat_features = self.categorical_features)\n",
    "            else: \n",
    "                train_dataset = cb.Pool(self.X_train_sets[i], self.y_train_sets[i])\n",
    "                \n",
    "            model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "                \n",
    "            grid = {'iterations': [100, 150, 200],\n",
    "                    'learning_rate': [0.03, 0.1],\n",
    "                    'depth': [2, 4, 6, 8],\n",
    "                    'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "            model.grid_search(grid, train_dataset, verbose=False)\n",
    "\n",
    "            self.models.append(model)\n",
    "        return self.models\n",
    "\n",
    "\n",
    "    def get_performance_multi_model_h2o(self):\n",
    "        mae_sum = 0\n",
    "\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            h2o_frame = h2o.H2OFrame(self.X_test_sets[i])\n",
    "\n",
    "            pred_h2o_frame = self.models[i].predict(h2o_frame)\n",
    "            pred_df = pred_h2o_frame.as_data_frame()\n",
    "            pred = pred_df['predict'].to_list()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], np.array(pred)))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"MAE: \", mae_sum)\n",
    "\n",
    "    def predict_multi_model_h2o(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            h2o_frame = h2o.H2OFrame(self.X_pred_sets[i])\n",
    "\n",
    "            pred_h2o_frame = self.models[i].predict(h2o_frame)\n",
    "            pred_df = pred_h2o_frame.as_data_frame()\n",
    "            X_pred = pred_df['predict'].to_list()\n",
    "            \n",
    "            unformated_pred = np.array(X_pred)\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.buildings[i]\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(np.array(unformated_pred))\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost ðŸ˜•\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def predict_multi_model(self):\n",
    "        preds = []\n",
    "        \n",
    "        for i in range(len(self.X_pred_sets)):\n",
    "            if self.feedback:\n",
    "                X_pred_without_feedback = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "                pred_list = []\n",
    "                # First feedback\n",
    "\n",
    "                \n",
    "                y_df = self.y_sets[i].to_frame()\n",
    "\n",
    "                feedback_init_row = shifted_values_24_h(y_df, \"pv_measurement\").iloc[-1]\n",
    "                \n",
    "                feedback_row = feedback_init_row\n",
    "\n",
    "                \n",
    "\n",
    "                for j, idx in enumerate(X_pred_without_feedback.index.to_list()):\n",
    "                   \n",
    "                    X_pred_with_feedback = pd.concat([X_pred_without_feedback.loc[idx].reset_index(drop=True), feedback_row.reset_index(drop=True)], axis=0, ignore_index=True)\n",
    "                    \n",
    "                    feedback = self.models[i].predict(X_pred_with_feedback)\n",
    "                    \n",
    "                    # Shift the values to the right\n",
    "                    feedback_row.iloc[1:] = feedback_row.iloc[:-1].values\n",
    "                    \n",
    "                    feedback_row.iloc[0] = feedback\n",
    "                    pred_list.append(feedback)\n",
    "\n",
    "                unformated_pred = np.array(pred_list)\n",
    "                \n",
    "                series = pd.Series(unformated_pred)\n",
    "                series.plot()\n",
    "                plt.show()\n",
    "\n",
    "            else:\n",
    "                X_pred = self.X_pred_sets[i].drop('date_forecast', axis = 1)\n",
    "                unformated_pred = self.models[i].predict(X_pred)\n",
    "\n",
    "            \n",
    "            unformated_pred_df = pd.DataFrame()\n",
    "            unformated_pred_df[\"date_forecast\"] = self.X_pred_sets[i][\"date_forecast\"]\n",
    "            unformated_pred_df[\"building\"] = self.buildings[i]\n",
    "\n",
    "            # Use the replace method with the specified column and dictionary\n",
    "            unformated_pred_df[\"pv_measurement\"] = pd.Series(np.array(unformated_pred))\n",
    "            unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "\n",
    "            unformated_pred_df[\"pv_measurement\"].plot()\n",
    "            plt.show()\n",
    "\n",
    "            preds.append(unformated_pred_df)\n",
    "\n",
    "        unformated_pred_df = pd.concat(preds, axis = 0, ignore_index = True)\n",
    "        # Should add a save method, so that not all work gets lost :/\n",
    "        \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def get_performance_multi_model(self) -> None:\n",
    "        mae_sum = 0\n",
    "        for i in range(len(self.X_test_sets)):\n",
    "            pred = self.models[i].predict(self.X_test_sets[i])\n",
    "            pd.Series(pred).plot()\n",
    "            pd.Series(self.y_test_sets[i]).plot()\n",
    "            plt.show()\n",
    "            mae = (mean_absolute_error(self.y_test_sets[i], np.array(pred)))\n",
    "            mae_sum = mae + mae_sum\n",
    "\n",
    "        print(\"Mean absolute error: \", mae_sum/len(self.X_test_sets))\n",
    "\n",
    "\n",
    "\n",
    "    def fit_model(self) -> None:\n",
    "        \"\"\"\n",
    "        Based on the selected model the class switches between what model is doing the learning. \n",
    "        \"\"\"\n",
    "\n",
    "        #============ SHOULD BE PLACED WITHIN A LIST OF FUNCTIONS ===================#\n",
    "        # Add a function that picks between different models, and processes the data based on this\n",
    "        train_dataset = cb.Pool(self.X_train, self.y_train, cat_features=['building'])\n",
    "\n",
    "        self.model = cb.CatBoostRegressor(loss_function=\"MAE\", logging_level='Silent')\n",
    "\n",
    "        grid = {'iterations': [100, 150, 200],\n",
    "                'learning_rate': [0.03, 0.1],\n",
    "                'depth': [2, 4, 6, 8],\n",
    "                'l2_leaf_reg': [0.2, 0.5, 1, 3]}\n",
    "\n",
    "        self.model.grid_search(grid, train_dataset, verbose=False)\n",
    "        \n",
    "\n",
    "    def get_performance(self) -> None:\n",
    "        pred = self.model.predict(self.X_test)\n",
    "        pd.Series(pred).plot()\n",
    "        pd.Series(self.y_test).plot()\n",
    "        \n",
    "        mae = (mean_absolute_error(self.y_test, pred))\n",
    "        print(\"Mean Abs: {:.2f}\".format(mae))\n",
    "\n",
    "    def predict(self) -> None:\n",
    "        \n",
    "        X_pred = self.X_pred.drop('date_forecast', axis = 1)\n",
    "        unformated_pred = self.model.predict(X_pred)\n",
    "        \n",
    "        unformated_pred_df = pd.DataFrame()\n",
    "        unformated_pred_df[\"date_forecast\"] = self.X_pred[\"date_forecast\"]\n",
    "        unformated_pred_df[\"building\"] = self.X_pred[\"building\"]\n",
    "\n",
    "        replace_dict = {0: 'A', 1: 'B', 2: 'C'}\n",
    "\n",
    "        # Use the replace method with the specified column and dictionary\n",
    "        unformated_pred_df[\"building\"] = unformated_pred_df[\"building\"].replace(replace_dict)\n",
    "        plot_ser = pd.Series(unformated_pred)\n",
    "        plot_ser.plot()\n",
    "        plt.show()\n",
    "        unformated_pred_df[\"pv_measurement\"] = pd.Series(unformated_pred)\n",
    "        unformated_pred_df[\"pv_measurement\"] = unformated_pred_df[\"pv_measurement\"].apply(lambda x: max(0, x))\n",
    "    \n",
    "        pred = self._format_predictions(unformated_pred_df)\n",
    "        self._save_predictions(pred)\n",
    "\n",
    "    def save_best_features(self, filename: str, N: int = 0):\n",
    "        if N == 0:\n",
    "            N = len(self.X_test.columns.values) - 1\n",
    "        best_features_df = pd.DataFrame()\n",
    "\n",
    "        feature_importance = self.model.get_feature_importance()\n",
    "\n",
    "        # Pair feature names with their importance scores\n",
    "        feature_importance_dict = dict(zip(self.model.feature_names_, feature_importance))\n",
    "\n",
    "        # Sort features by importance\n",
    "        sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Print or use the top features\n",
    "        top_features = sorted_feature_importance[:N]  # Replace N with the number of top features you want\n",
    "        \n",
    "        # Saving to list\n",
    "        labels = list(self.X_test.columns.values)\n",
    "        best_features = []\n",
    "\n",
    "        for feat in top_features:\n",
    "            best_features.append(feat[0])\n",
    "        \n",
    "        best_features_df[\"Model\"] = pd.Series(np.array(best_features))\n",
    "\n",
    "        best_features_df.to_csv(\"tests/\" + self.save_folder + \"single_learner\" + '.csv', sep ='\\t')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_best_features_multi_model(self, filename: str, N: int = 0):\n",
    "        if N == 0:\n",
    "            N = len(self.X_test_sets[0].columns.values) - 1\n",
    "        best_features_df = pd.DataFrame()\n",
    "\n",
    "        for i, X in enumerate(self.X_test_sets):\n",
    "           \n",
    "            feature_importance = self.models[i].get_feature_importance()\n",
    "\n",
    "            # Pair feature names with their importance scores\n",
    "            feature_importance_dict = dict(zip(self.models[i].feature_names_, feature_importance))\n",
    "\n",
    "            # Sort features by importance\n",
    "            sorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # Print or use the top features\n",
    "            top_features = sorted_feature_importance[:N]  # Replace N with the number of top features you want\n",
    "            \n",
    "            # Saving to list\n",
    "            labels = list(X.columns.values)\n",
    "            best_features = []\n",
    "\n",
    "            for feat in top_features:\n",
    "                best_features.append(feat[0])\n",
    "            \n",
    "            best_features_df[\"Model \" + self.buildings[i]] = pd.Series(np.array(best_features))\n",
    "\n",
    "        best_features_df.to_csv(\"tests/\" + self.save_folder + filename + '.csv', sep ='\\t')\n",
    "\n",
    "\n",
    "    def _format_predictions(self, unformated_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # \n",
    "        to_be_submitted_index = pd.read_csv(\"test.csv\")\n",
    "\n",
    "        #convert the \"time\" column to datetime\n",
    "        to_be_submitted_index[\"time\"] = pd.to_datetime(to_be_submitted_index[\"time\"])\n",
    "        pred = pd.merge(unformated_pred, to_be_submitted_index, how='inner', left_on=['date_forecast', 'building'], right_on=[\"time\", \"location\"])\n",
    "        \n",
    "        return pred\n",
    "        \n",
    "        \n",
    "    def _save_predictions(self, pred: pd.DataFrame)->None:\n",
    "        #Make the index and pv_measurement column into a csv file\n",
    "        pred[[\"id\", \"pv_measurement\"]].rename(columns={\"id\" : \"id\" , \"pv_measurement\" : \"prediction\"}).to_csv(\"tests/\" + self.save_folder + \"model_pred.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_with_too_many_NaN = ['ceiling_height_agl:m', 'cloud base agl:m', 'snow density:km3']\n",
    "features_with_bad_qualities = ['elevation:m', 'fresh_snow_3h:cm', 'wind_speed_w_1000hPa:ms', 'snow_drift:idx', 'fresh_snow_12h:cm', 'fresh_snow_24h:cm', 'fresh_snow_1h:cm', 'snow_depth:cm', 'fresh_snow_6h:cm', 'snow_melt_10min:mm']\n",
    "categorical_features = ['dew_or_rime:idx'] # 'is_day:idx', 'is_in_shadow:idx', \n",
    "\n",
    "features = default_features\n",
    "features = [item for item in features if item not in features_with_too_many_NaN]\n",
    "features = [item for item in features if item not in features_with_bad_qualities]\n",
    "\n",
    "dervative_features = [\"derivative_\" + item + \"_d\" for item in features]\n",
    "\n",
    "features = features + derivative_features + time_features\n",
    "\n",
    "cat_features = [] #categorical_features\n",
    "\n",
    "multi_learner = learner(file_paths = file_paths, features = features, categorical_features = categorical_features)\n",
    "multi_learner.create_training_data_multi_model()\n",
    "multi_learner.fit_multi_model_h2o()\n",
    "multi_learner.predict_multi_model_h2o()\n",
    "\n",
    "# Iterate thorugh several learners and add them to the "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
